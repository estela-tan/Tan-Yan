[
  {
    "objectID": "Lecture/index.html",
    "href": "Lecture/index.html",
    "title": "Homeworks",
    "section": "",
    "text": "homework 1 for lecture 3\n\n\nhomework 2 for lecture 3\n\n\nhomework for lecture 2-pet names\n\n\nhomework for lecture 4\n\n\nhomework for lecture 5"
  },
  {
    "objectID": "Lecture/homework for lecture 4.html",
    "href": "Lecture/homework for lecture 4.html",
    "title": "Tan Yan website",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv('data/all-ages.csv')\n\n\ndf\n\n\n\n\n\n\n\n\nMajor_code\nMajor\nMajor_category\nTotal\nEmployed\nEmployed_full_time_year_round\nUnemployed\nUnemployment_rate\nMedian\nP25th\nP75th\n\n\n\n\n0\n1100\nGENERAL AGRICULTURE\nAgriculture & Natural Resources\n128148\n90245\n74078\n2423\n0.026147\n50000\n34000\n80000.0\n\n\n1\n1101\nAGRICULTURE PRODUCTION AND MANAGEMENT\nAgriculture & Natural Resources\n95326\n76865\n64240\n2266\n0.028636\n54000\n36000\n80000.0\n\n\n2\n1102\nAGRICULTURAL ECONOMICS\nAgriculture & Natural Resources\n33955\n26321\n22810\n821\n0.030248\n63000\n40000\n98000.0\n\n\n3\n1103\nANIMAL SCIENCES\nAgriculture & Natural Resources\n103549\n81177\n64937\n3619\n0.042679\n46000\n30000\n72000.0\n\n\n4\n1104\nFOOD SCIENCE\nAgriculture & Natural Resources\n24280\n17281\n12722\n894\n0.049188\n62000\n38500\n90000.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n6211\nHOSPITALITY MANAGEMENT\nBusiness\n200854\n163393\n122499\n8862\n0.051447\n49000\n33000\n70000.0\n\n\n169\n6212\nMANAGEMENT INFORMATION SYSTEMS AND STATISTICS\nBusiness\n156673\n134478\n118249\n6186\n0.043977\n72000\n50000\n100000.0\n\n\n170\n6299\nMISCELLANEOUS BUSINESS & MEDICAL ADMINISTRATION\nBusiness\n102753\n77471\n61603\n4308\n0.052679\n53000\n36000\n83000.0\n\n\n171\n6402\nHISTORY\nHumanities & Liberal Arts\n712509\n478416\n354163\n33725\n0.065851\n50000\n35000\n80000.0\n\n\n172\n6403\nUNITED STATES HISTORY\nHumanities & Liberal Arts\n17746\n11887\n8204\n943\n0.073500\n50000\n39000\n81000.0\n\n\n\n\n173 rows × 11 columns\n\n\n\n\n# 按照专业分组，并把失业率从低到高升序排列\nresult = df.groupby([\"Major\"]).sum().sort_values([\"Unemployment_rate\"])\nprint(result)\n\n                                            Major_code  \\\nMajor                                                    \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING            2411   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION        2301   \nPHARMACOLOGY                                      3607   \nMATERIALS SCIENCE                                 5008   \nMATHEMATICS AND COMPUTER SCIENCE                  4005   \n...                                                ...   \nLIBRARY SCIENCE                                   3501   \nSCHOOL STUDENT COUNSELING                         2303   \nMILITARY TECHNOLOGIES                             3801   \nCLINICAL PSYCHOLOGY                               5202   \nMISCELLANEOUS FINE ARTS                           6099   \n\n                                                                 Major_category  \\\nMajor                                                                             \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                              Engineering   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                            Education   \nPHARMACOLOGY                                             Biology & Life Science   \nMATERIALS SCIENCE                                                   Engineering   \nMATHEMATICS AND COMPUTER SCIENCE                        Computers & Mathematics   \n...                                                                         ...   \nLIBRARY SCIENCE                                                       Education   \nSCHOOL STUDENT COUNSELING                                             Education   \nMILITARY TECHNOLOGIES                       Industrial Arts & Consumer Services   \nCLINICAL PSYCHOLOGY                                    Psychology & Social Work   \nMISCELLANEOUS FINE ARTS                                                    Arts   \n\n                                            Total  Employed  \\\nMajor                                                         \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       6264      4120   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   4037      3113   \nPHARMACOLOGY                                 5015      3481   \nMATERIALS SCIENCE                            7208      5866   \nMATHEMATICS AND COMPUTER SCIENCE             7184      5874   \n...                                           ...       ...   \nLIBRARY SCIENCE                             16193      7091   \nSCHOOL STUDENT COUNSELING                    2396      1492   \nMILITARY TECHNOLOGIES                        4315      1650   \nCLINICAL PSYCHOLOGY                          7638      5128   \nMISCELLANEOUS FINE ARTS                      8511      6431   \n\n                                            Employed_full_time_year_round  \\\nMajor                                                                       \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                               3350   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                           2468   \nPHARMACOLOGY                                                         2579   \nMATERIALS SCIENCE                                                    4505   \nMATHEMATICS AND COMPUTER SCIENCE                                     5039   \n...                                                                   ...   \nLIBRARY SCIENCE                                                      4330   \nSCHOOL STUDENT COUNSELING                                            1093   \nMILITARY TECHNOLOGIES                                                1708   \nCLINICAL PSYCHOLOGY                                                  3297   \nMISCELLANEOUS FINE ARTS                                              3802   \n\n                                            Unemployed  Unemployment_rate  \\\nMajor                                                                       \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING               0           0.000000   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION           0           0.000000   \nPHARMACOLOGY                                        57           0.016111   \nMATERIALS SCIENCE                                  134           0.022333   \nMATHEMATICS AND COMPUTER SCIENCE                   150           0.024900   \n...                                                ...                ...   \nLIBRARY SCIENCE                                    743           0.094843   \nSCHOOL STUDENT COUNSELING                          169           0.101746   \nMILITARY TECHNOLOGIES                              187           0.101796   \nCLINICAL PSYCHOLOGY                                587           0.102712   \nMISCELLANEOUS FINE ARTS                           1190           0.156147   \n\n                                            Median  P25th     P75th  \nMajor                                                                \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       85000  55000  125000.0  \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   58000  44750   79000.0  \nPHARMACOLOGY                                 60000  35000  105000.0  \nMATERIALS SCIENCE                            75000  60000  100000.0  \nMATHEMATICS AND COMPUTER SCIENCE             92000  53000  136000.0  \n...                                            ...    ...       ...  \nLIBRARY SCIENCE                              40000  30000   55000.0  \nSCHOOL STUDENT COUNSELING                    41000  33200   50000.0  \nMILITARY TECHNOLOGIES                        64000  39750   90000.0  \nCLINICAL PSYCHOLOGY                          45000  26100   62000.0  \nMISCELLANEOUS FINE ARTS                      45000  30000   60000.0  \n\n[173 rows x 10 columns]\n\n\n\nimport pandas as pd\ndf = pd.read_csv('recent-grads.csv')\ndf\n\n\n\n\n\n\n\n\nRank\nMajor_code\nMajor\nTotal\nMen\nWomen\nMajor_category\nShareWomen\nSample_size\nEmployed\n...\nPart_time\nFull_time_year_round\nUnemployed\nUnemployment_rate\nMedian\nP25th\nP75th\nCollege_jobs\nNon_college_jobs\nLow_wage_jobs\n\n\n\n\n0\n1\n2419\nPETROLEUM ENGINEERING\n2339.0\n2057.0\n282.0\nEngineering\n0.120564\n36\n1976\n...\n270\n1207\n37\n0.018381\n110000\n95000\n125000\n1534\n364\n193\n\n\n1\n2\n2416\nMINING AND MINERAL ENGINEERING\n756.0\n679.0\n77.0\nEngineering\n0.101852\n7\n640\n...\n170\n388\n85\n0.117241\n75000\n55000\n90000\n350\n257\n50\n\n\n2\n3\n2415\nMETALLURGICAL ENGINEERING\n856.0\n725.0\n131.0\nEngineering\n0.153037\n3\n648\n...\n133\n340\n16\n0.024096\n73000\n50000\n105000\n456\n176\n0\n\n\n3\n4\n2417\nNAVAL ARCHITECTURE AND MARINE ENGINEERING\n1258.0\n1123.0\n135.0\nEngineering\n0.107313\n16\n758\n...\n150\n692\n40\n0.050125\n70000\n43000\n80000\n529\n102\n0\n\n\n4\n5\n2405\nCHEMICAL ENGINEERING\n32260.0\n21239.0\n11021.0\nEngineering\n0.341631\n289\n25694\n...\n5180\n16697\n1672\n0.061098\n65000\n50000\n75000\n18314\n4440\n972\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n169\n3609\nZOOLOGY\n8409.0\n3050.0\n5359.0\nBiology & Life Science\n0.637293\n47\n6259\n...\n2190\n3602\n304\n0.046320\n26000\n20000\n39000\n2771\n2947\n743\n\n\n169\n170\n5201\nEDUCATIONAL PSYCHOLOGY\n2854.0\n522.0\n2332.0\nPsychology & Social Work\n0.817099\n7\n2125\n...\n572\n1211\n148\n0.065112\n25000\n24000\n34000\n1488\n615\n82\n\n\n170\n171\n5202\nCLINICAL PSYCHOLOGY\n2838.0\n568.0\n2270.0\nPsychology & Social Work\n0.799859\n13\n2101\n...\n648\n1293\n368\n0.149048\n25000\n25000\n40000\n986\n870\n622\n\n\n171\n172\n5203\nCOUNSELING PSYCHOLOGY\n4626.0\n931.0\n3695.0\nPsychology & Social Work\n0.798746\n21\n3777\n...\n965\n2738\n214\n0.053621\n23400\n19200\n26000\n2403\n1245\n308\n\n\n172\n173\n3501\nLIBRARY SCIENCE\n1098.0\n134.0\n964.0\nEducation\n0.877960\n2\n742\n...\n237\n410\n87\n0.104946\n22000\n20000\n22000\n288\n338\n192\n\n\n\n\n173 rows × 21 columns\n\n\n\n\n# 按照专业分组，将女生占比从高到低降序排列\nresult = df.groupby([\"Major\"]).sum().sort_values([\"ShareWomen\"],ascending=False)\nprint(result)\n\n                                               Rank  Major_code     Total  \\\nMajor                                                                       \nEARLY CHILDHOOD EDUCATION                       165        2307   37589.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   164        6102   38279.0   \nMEDICAL ASSISTING SERVICES                       52        6104   11123.0   \nELEMENTARY EDUCATION                            139        2304  170862.0   \nFAMILY AND CONSUMER SCIENCES                    151        2901   58001.0   \n...                                             ...         ...       ...   \nMINING AND MINERAL ENGINEERING                    2        2416     756.0   \nCONSTRUCTION SERVICES                            27        5601   18498.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      67        2504    4790.0   \nMILITARY TECHNOLOGIES                            74        3801     124.0   \nFOOD SCIENCE                                     22        1104       0.0   \n\n                                                   Men     Women  \\\nMajor                                                              \nEARLY CHILDHOOD EDUCATION                       1167.0   36422.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   1225.0   37054.0   \nMEDICAL ASSISTING SERVICES                       803.0   10320.0   \nELEMENTARY EDUCATION                           13029.0  157833.0   \nFAMILY AND CONSUMER SCIENCES                    5166.0   52835.0   \n...                                                ...       ...   \nMINING AND MINERAL ENGINEERING                   679.0      77.0   \nCONSTRUCTION SERVICES                          16820.0    1678.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     4419.0     371.0   \nMILITARY TECHNOLOGIES                            124.0       0.0   \nFOOD SCIENCE                                       0.0       0.0   \n\n                                                                    Major_category  \\\nMajor                                                                                \nEARLY CHILDHOOD EDUCATION                                                Education   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                               Health   \nMEDICAL ASSISTING SERVICES                                                  Health   \nELEMENTARY EDUCATION                                                     Education   \nFAMILY AND CONSUMER SCIENCES                   Industrial Arts & Consumer Services   \n...                                                                            ...   \nMINING AND MINERAL ENGINEERING                                         Engineering   \nCONSTRUCTION SERVICES                          Industrial Arts & Consumer Services   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                            Engineering   \nMILITARY TECHNOLOGIES                          Industrial Arts & Consumer Services   \nFOOD SCIENCE                                       Agriculture & Natural Resources   \n\n                                               ShareWomen  Sample_size  \\\nMajor                                                                    \nEARLY CHILDHOOD EDUCATION                        0.968954          342   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES    0.967998           95   \nMEDICAL ASSISTING SERVICES                       0.927807           67   \nELEMENTARY EDUCATION                             0.923745         1629   \nFAMILY AND CONSUMER SCIENCES                     0.910933          518   \n...                                                   ...          ...   \nMINING AND MINERAL ENGINEERING                   0.101852            7   \nCONSTRUCTION SERVICES                            0.090713          295   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      0.077453           71   \nMILITARY TECHNOLOGIES                            0.000000            4   \nFOOD SCIENCE                                     0.000000           36   \n\n                                               Employed  Full_time  Part_time  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                         32551      27569       7001   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES     29763      19975      13862   \nMEDICAL ASSISTING SERVICES                         9168       5643       4107   \nELEMENTARY EDUCATION                             149339     123177      37965   \nFAMILY AND CONSUMER SCIENCES                      46624      36747      15872   \n...                                                 ...        ...        ...   \nMINING AND MINERAL ENGINEERING                      640        556        170   \nCONSTRUCTION SERVICES                             16318      15690       1751   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES        4186       4175        247   \nMILITARY TECHNOLOGIES                                 0        111          0   \nFOOD SCIENCE                                       3149       2558       1121   \n\n                                               Full_time_year_round  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                                     20748   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                 14460   \nMEDICAL ASSISTING SERVICES                                     4290   \nELEMENTARY EDUCATION                                          86540   \nFAMILY AND CONSUMER SCIENCES                                  26906   \n...                                                             ...   \nMINING AND MINERAL ENGINEERING                                  388   \nCONSTRUCTION SERVICES                                         12313   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                    3607   \nMILITARY TECHNOLOGIES                                           111   \nFOOD SCIENCE                                                   1735   \n\n                                               Unemployed  Unemployment_rate  \\\nMajor                                                                          \nEARLY CHILDHOOD EDUCATION                            1360           0.040105   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES        1487           0.047584   \nMEDICAL ASSISTING SERVICES                            407           0.042507   \nELEMENTARY EDUCATION                                 7297           0.046586   \nFAMILY AND CONSUMER SCIENCES                         3355           0.067128   \n...                                                   ...                ...   \nMINING AND MINERAL ENGINEERING                         85           0.117241   \nCONSTRUCTION SERVICES                                1042           0.060023   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES           250           0.056357   \nMILITARY TECHNOLOGIES                                   0           0.000000   \nFOOD SCIENCE                                          338           0.096931   \n\n                                               Median  P25th  P75th  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                       28000  21000  35000   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   28000  20000  40000   \nMEDICAL ASSISTING SERVICES                      42000  30000  65000   \nELEMENTARY EDUCATION                            32000  23400  38000   \nFAMILY AND CONSUMER SCIENCES                    30000  22900  40000   \n...                                               ...    ...    ...   \nMINING AND MINERAL ENGINEERING                  75000  55000  90000   \nCONSTRUCTION SERVICES                           50000  36000  60000   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     40000  27000  52000   \nMILITARY TECHNOLOGIES                           40000  40000  40000   \nFOOD SCIENCE                                    53000  32000  70000   \n\n                                               College_jobs  Non_college_jobs  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                             23515              7705   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES         19957              9404   \nMEDICAL ASSISTING SERVICES                             2091              6948   \nELEMENTARY EDUCATION                                 108085             36972   \nFAMILY AND CONSUMER SCIENCES                          20985             20133   \n...                                                     ...               ...   \nMINING AND MINERAL ENGINEERING                          350               257   \nCONSTRUCTION SERVICES                                  3275              5351   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES            1861              2121   \nMILITARY TECHNOLOGIES                                     0                 0   \nFOOD SCIENCE                                           1183              1274   \n\n                                               Low_wage_jobs  \nMajor                                                         \nEARLY CHILDHOOD EDUCATION                               2868  \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES           5125  \nMEDICAL ASSISTING SERVICES                              1270  \nELEMENTARY EDUCATION                                   11502  \nFAMILY AND CONSUMER SCIENCES                            5248  \n...                                                      ...  \nMINING AND MINERAL ENGINEERING                            50  \nCONSTRUCTION SERVICES                                    703  \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES              406  \nMILITARY TECHNOLOGIES                                      0  \nFOOD SCIENCE                                             485  \n\n[173 rows x 20 columns]\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\na=df['Median'].groupby(df['Major_category']).sum()\na.plot.bar()\nplt.show()",
    "crumbs": [
      "Exercises",
      "Labs",
      "homework for lecture 4"
    ]
  },
  {
    "objectID": "Lecture/homework 2 for lecture 3.html",
    "href": "Lecture/homework 2 for lecture 3.html",
    "title": "Tan Yan website",
    "section": "",
    "text": "import pandas as pd\nurl ='https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\ndf = pd.read_csv(url)\nprint(df.head())\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\n\n\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\nfrom skimpy import clean_columns\ndf = clean_columns(df,case=\"snake\")\nprint(df.columns)\n\nIndex(['passenger_id', 'survived', 'pclass', 'name', 'sex', 'age', 'sib_sp',\n       'parch', 'ticket', 'fare', 'cabin', 'embarked'],\n      dtype='object')\n\n\n\ndf.fillna(\"-\")\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nname\nsex\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\n-\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\n-\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\n-\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\n-\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\n-\n1\n2\nW./C. 6607\n23.4500\n-\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\n-\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nsib_sp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\nsum_table = df.describe().round(2)\nsum_table\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nsib_sp\nparch\nfare\n\n\n\n\ncount\n891.00\n891.00\n891.00\n891.00\n891.00\n891.00\n\n\nmean\n446.00\n0.38\n2.31\n0.52\n0.38\n32.20\n\n\nstd\n257.35\n0.49\n0.84\n1.10\n0.81\n49.69\n\n\nmin\n1.00\n0.00\n1.00\n0.00\n0.00\n0.00\n\n\n25%\n223.50\n0.00\n2.00\n0.00\n0.00\n7.91\n\n\n50%\n446.00\n0.00\n3.00\n0.00\n0.00\n14.45\n\n\n75%\n668.50\n1.00\n3.00\n1.00\n0.00\n31.00\n\n\nmax\n891.00\n1.00\n3.00\n8.00\n6.00\n512.33\n\n\n\n\n\n\n\n\ndf.dropna()\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nname\nsex\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\n-\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\n-\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\n-\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\n-\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\n-\n1\n2\nW./C. 6607\n23.4500\n-\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\n-\nQ\n\n\n\n\n891 rows × 12 columns",
    "crumbs": [
      "Exercises",
      "Labs",
      "homework 2 for lecture 3"
    ]
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "lab2",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n \nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)",
    "crumbs": [
      "Exercises",
      "Lecture",
      "lab2"
    ]
  },
  {
    "objectID": "labs/lab2.html#introduction",
    "href": "labs/lab2.html#introduction",
    "title": "lab2",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n \nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)",
    "crumbs": [
      "Exercises",
      "Lecture",
      "lab2"
    ]
  },
  {
    "objectID": "labs/lab1-6 Chipotle .html",
    "href": "labs/lab1-6 Chipotle .html",
    "title": "Visualizing Chipotle’s Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set this so the graphs open internally\n%matplotlib inline\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n    \nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. Create a histogram of the top 5 items bought\n\n# get the Series of the names\nx = chipo.item_name\n\n# use the Counter class from collections to create a dictionary with keys(text) and frequency\nletter_counts = Counter(x)\n\n# convert the dictionary to a DataFrame\ndf = pd.DataFrame.from_dict(letter_counts, orient='index')\n\n# sort the values from the top to the least value and slice the first 5 items\ndf = df[0].sort_values(ascending = True)[45:50]\n\n# create the plot\ndf.plot(kind='bar')\n\n# Set the title and labels\nplt.xlabel('Items')\nplt.ylabel('Number of Times Ordered')\nplt.title('Most ordered Chipotle\\'s Items')\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStep 6. Create a scatterplot with the number of items orderered per order price\n\nHint: Price should be in the X-axis and Items ordered in the Y-axis\n\nchipo.item_price = [float(value[1:-1]) for value in chipo.item_price] # strip the dollar sign and trailing space\n\n# then groupby the orders and sum\norders = chipo.groupby('order_id').sum()\n\n# creates the scatterplot\n# plt.scatter(orders.quantity, orders.item_price, s = 50, c = 'green')\nplt.scatter(x = orders.item_price, y = orders.quantity, s = 50, c = 'green')\n\n# Set the title and labels\nplt.xlabel('Order Price')\nplt.ylabel('Items ordered')\nplt.title('Number of items ordered per order price')\nplt.ylim(0)\n\n\n\n\n\n\n\n\n\n\n\nStep 7. BONUS: Create a question and a graph to answer your own question.\n\n# Load the Chipotle dataset\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\nchipo = pd.read_csv(url, sep = '\\t')\n\n# Strip the dollar sign and convert item_price to float\nchipo.item_price = [float(value[1:-1]) for value in chipo.item_price]\n\n# Step 1: Sort the items by price (descending)\nmost_expensive_items = chipo.groupby('item_name').item_price.mean().sort_values(ascending=False).head(10)\n\n# Step 2: Get the order quantity for these most expensive items\ntop_items = chipo[chipo['item_name'].isin(most_expensive_items.index)]\nitem_order_counts = top_items.groupby('item_name')['quantity'].sum()\n\n# Step 3: Create the plot to visualize the most expensive items and their quantities\nfig, ax = plt.subplots(figsize=(10, 6))\nitem_order_counts.plot(kind='bar', ax=ax, color='skyblue')\n\n# Add labels and title\nax.set_xlabel('Menu Item')\nax.set_ylabel('Total Quantity Ordered')\nax.set_title('Top 10 Most Expensive Menu Items and Their Order Quantities')\n\n# Show the plot\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()",
    "crumbs": [
      "Exercises",
      "Lecture",
      "lab1-6 Chipotle"
    ]
  },
  {
    "objectID": "labs/lab1-4 chipotle .html",
    "href": "labs/lab1-4 chipotle .html",
    "title": "Ex1 - Filtering and Sorting Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n    \nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. How many products cost more than $10.00?\n\n# Step 1: Clean the 'item_price' column and convert it to a float\nchipo['item_price'] = chipo['item_price'].replace({'\\$': ''}, regex=True).astype(float)\n\n# Step 2: Remove duplicates based on 'item_name' and 'quantity'\nchipo_cleaned = chipo.drop_duplicates(subset=['item_name', 'quantity'])\n\n# Step 3: Filter the dataset to include only rows where 'quantity' equals 1\nchipo_single_item = chipo_cleaned[chipo_cleaned['quantity'] == 1]\n\n# Step 4: Find how many products cost more than $10.00\nexpensive_products = chipo_single_item[chipo_single_item['item_price'] &gt; 10.00]\n\n# Count the number of unique products that cost more than $10.00\nnum_expensive_products = expensive_products['item_name'].nunique()\n\n# Print the result\nprint(f\"{num_expensive_products}\")\n\n12\n\n\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\$'\n&lt;&gt;:2: SyntaxWarning: invalid escape sequence '\\$'\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6956\\2090063564.py:2: SyntaxWarning: invalid escape sequence '\\$'\n  chipo['item_price'] = chipo['item_price'].replace({'\\$': ''}, regex=True).astype(float)\n\n\n\n\nStep 5. What is the price of each item?\n\nprint a data frame with only two columns item_name and item_price\n\n# Step 1: Remove duplicates based on 'item_name' and 'quantity'\nchipo_cleaned = chipo.drop_duplicates(subset=['item_name', 'quantity'])\n\n# Step 2: Select only the rows where 'quantity' equals 1\nchipo_single_item = chipo_cleaned[chipo_cleaned['quantity'] == 1]\n\n# Step 3: Select only the 'item_name' and 'item_price' columns\nchipo_price_df = chipo_single_item[['item_name', 'item_price']]\n\n# Step 4: Sort the values by 'item_price' from most to least expensive\nchipo_price_df_sorted = chipo_price_df.sort_values(by='item_price', ascending=False)\n\n# Print the resulting DataFrame\nprint(chipo_price_df_sorted)\n\n                                  item_name  item_price\n1132                    Carnitas Salad Bowl       11.89\n1229                    Barbacoa Salad Bowl       11.89\n606                        Steak Salad Bowl       11.89\n39                            Barbacoa Bowl       11.75\n7                             Steak Burrito       11.75\n168                   Barbacoa Crispy Tacos       11.75\n57                           Veggie Burrito       11.25\n62                              Veggie Bowl       11.25\n186                       Veggie Salad Bowl       11.25\n738                       Veggie Soft Tacos       11.25\n250                           Chicken Salad       10.98\n5                              Chicken Bowl       10.98\n8                          Steak Soft Tacos        9.25\n92                       Steak Crispy Tacos        9.25\n554                   Carnitas Crispy Tacos        9.25\n237                     Carnitas Soft Tacos        9.25\n56                      Barbacoa Soft Tacos        9.25\n33                            Carnitas Bowl        8.99\n664                             Steak Salad        8.99\n21                         Barbacoa Burrito        8.99\n3750                         Carnitas Salad        8.99\n54                               Steak Bowl        8.99\n27                         Carnitas Burrito        8.99\n44                       Chicken Salad Bowl        8.75\n11                     Chicken Crispy Tacos        8.75\n12                       Chicken Soft Tacos        8.75\n1653                    Veggie Crispy Tacos        8.49\n16                          Chicken Burrito        8.49\n1694                           Veggie Salad        8.49\n1414                                  Salad        7.40\n673                                    Bowl        7.40\n520                            Crispy Tacos        7.40\n510                                 Burrito        7.40\n298                       6 Pack Soft Drink        6.49\n10                      Chips and Guacamole        4.45\n2                          Nantucket Nectar        3.39\n1                                      Izze        3.39\n674       Chips and Mild Fresh Tomato Salsa        3.00\n233      Chips and Roasted Chili Corn Salsa        2.95\n38    Chips and Tomatillo Green Chili Salsa        2.95\n111     Chips and Tomatillo Red Chili Salsa        2.95\n0              Chips and Fresh Tomato Salsa        2.39\n191      Chips and Roasted Chili-Corn Salsa        2.39\n300     Chips and Tomatillo-Red Chili Salsa        2.39\n3     Chips and Tomatillo-Green Chili Salsa        2.39\n40                                    Chips        2.15\n6                             Side of Chips        1.69\n263                       Canned Soft Drink        1.25\n34                            Bottled Water        1.09\n28                              Canned Soda        1.09\n\n\n\n\n\nStep 6. Sort by the name of the item\n\n # OR\nsorted_df = chipo.sort_values(by='item_name') \npd.set_option('display.width', 400) \n# 显示包含指定列的整个排序后的DataFrame\nprint(sorted_df[['order_id', 'quantity', 'item_name', 'choice_description', 'item_price']])\n\n\n      order_id  quantity          item_name                                 choice_description  item_price\n3389      1360         2  6 Pack Soft Drink                                        [Diet Coke]       12.98\n341        148         1  6 Pack Soft Drink                                        [Diet Coke]        6.49\n1849       749         1  6 Pack Soft Drink                                             [Coke]        6.49\n1860       754         1  6 Pack Soft Drink                                        [Diet Coke]        6.49\n2713      1076         1  6 Pack Soft Drink                                             [Coke]        6.49\n...        ...       ...                ...                                                ...         ...\n2384       948         1  Veggie Soft Tacos  [Roasted Chili Corn Salsa, [Fajita Vegetables,...        8.75\n781        322         1  Veggie Soft Tacos  [Fresh Tomato Salsa, [Black Beans, Cheese, Sou...        8.75\n2851      1132         1  Veggie Soft Tacos  [Roasted Chili Corn Salsa (Medium), [Black Bea...        8.49\n1699       688         1  Veggie Soft Tacos  [Fresh Tomato Salsa, [Fajita Vegetables, Rice,...       11.25\n1395       567         1  Veggie Soft Tacos  [Fresh Tomato Salsa (Mild), [Pinto Beans, Rice...        8.49\n\n[4622 rows x 5 columns]\n\n\n\n\nStep 7. What was the quantity of the most expensive item ordered?\n\nmost_expensive_item = chipo.sort_values(by='item_price', ascending=False).head(1)\n \n# 选择需要的列\nselected_columns = ['order_id', 'quantity', 'item_name', 'choice_description', 'item_price']\nmost_expensive_item_info = most_expensive_item[selected_columns]\n\n \n# 输出结果\nprint(most_expensive_item_info)\n\n      order_id  quantity                     item_name choice_description  item_price\n3598      1443        15  Chips and Fresh Tomato Salsa                NaN       44.25\n\n\n\n\nStep 8. How many times was a Veggie Salad Bowl ordered?\n\n# Step 8: Count how many times 'Veggie Salad Bowl' was ordered\nveggie_salad_bowl_count = chipo[chipo['item_name'] == 'Veggie Salad Bowl'].shape[0]\n\n# Print the result\nprint( veggie_salad_bowl_count)\n\n18\n\n\n\n\nStep 9. How many times did someone order more than one Canned Soda?\n\n\ncanned_soda_multiple_order = chipo[(chipo['item_name'] == 'Canned Soda') & (chipo['quantity'] &gt; 1)]\ncanned_soda_multiple_order_count = canned_soda_multiple_order.shape[0]\nprint(canned_soda_multiple_order_count)\n\n20",
    "crumbs": [
      "Exercises",
      "Lecture",
      "lab1-4 chipotle"
    ]
  },
  {
    "objectID": "labs/lab1-2 Occupation.html",
    "href": "labs/lab1-2 Occupation.html",
    "title": "Ex3 - Getting and Knowing your Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called users and use the ‘user_id’ as index\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user'\n\n# Load the dataset into a DataFrame\nuser_data = pd.read_csv(url, delimiter='|')\n\n\n\nStep 4. See the first 25 entries\n\ndf = pd.read_csv(url,sep='|')\nusers = df\nusers.set_index('user_id')\nusers.head(25)\n\n\n\n\n\n\n\n\nuser_id\nage\ngender\noccupation\nzip_code\n\n\n\n\n0\n1\n24\nM\ntechnician\n85711\n\n\n1\n2\n53\nF\nother\n94043\n\n\n2\n3\n23\nM\nwriter\n32067\n\n\n3\n4\n24\nM\ntechnician\n43537\n\n\n4\n5\n33\nF\nother\n15213\n\n\n5\n6\n42\nM\nexecutive\n98101\n\n\n6\n7\n57\nM\nadministrator\n91344\n\n\n7\n8\n36\nM\nadministrator\n05201\n\n\n8\n9\n29\nM\nstudent\n01002\n\n\n9\n10\n53\nM\nlawyer\n90703\n\n\n10\n11\n39\nF\nother\n30329\n\n\n11\n12\n28\nF\nother\n06405\n\n\n12\n13\n47\nM\neducator\n29206\n\n\n13\n14\n45\nM\nscientist\n55106\n\n\n14\n15\n49\nF\neducator\n97301\n\n\n15\n16\n21\nM\nentertainment\n10309\n\n\n16\n17\n30\nM\nprogrammer\n06355\n\n\n17\n18\n35\nF\nother\n37212\n\n\n18\n19\n40\nM\nlibrarian\n02138\n\n\n19\n20\n42\nF\nhomemaker\n95660\n\n\n20\n21\n26\nM\nwriter\n30068\n\n\n21\n22\n25\nM\nwriter\n40206\n\n\n22\n23\n30\nF\nartist\n48197\n\n\n23\n24\n21\nF\nartist\n94533\n\n\n24\n25\n39\nM\nengineer\n55107\n\n\n\n\n\n\n\n\n\nStep 5. See the last 10 entries\n\nusers.tail(10)\n\n\n\n\n\n\n\n\nuser_id\nage\ngender\noccupation\nzip_code\n\n\n\n\n933\n934\n61\nM\nengineer\n22902\n\n\n934\n935\n42\nM\ndoctor\n66221\n\n\n935\n936\n24\nM\nother\n32789\n\n\n936\n937\n48\nM\neducator\n98072\n\n\n937\n938\n38\nF\ntechnician\n55038\n\n\n938\n939\n26\nF\nstudent\n33319\n\n\n939\n940\n32\nM\nadministrator\n02215\n\n\n940\n941\n20\nM\nstudent\n97229\n\n\n941\n942\n48\nF\nlibrarian\n78209\n\n\n942\n943\n22\nM\nstudent\n77841\n\n\n\n\n\n\n\n\n\nStep 6. What is the number of observations in the dataset?\n\nnum_rows = len(users)\nprint(f\"{num_rows}\")\n\n943\n\n\n\n\nStep 7. What is the number of columns in the dataset?\n\nnum_columns = len(users.columns)\nprint(f\"{num_columns}\")\n\n5\n\n\n\n\nStep 8. Print the name of all the columns.\n\nusers.columns\n\nIndex(['user_id', 'age', 'gender', 'occupation', 'zip_code'], dtype='object')\n\n\n\n\nStep 9. How is the dataset indexed?\n\nusers.index\n\nRangeIndex(start=0, stop=943, step=1)\n\n\n\n\nStep 10. What is the data type of each column?\n\nnum_rows = len(df)\ndtypes = df.dtypes\nprint(dtypes.to_string(header=False, index=True))\n\nuser_id        int64\nage            int64\ngender        object\noccupation    object\nzip_code      object\n\n\n\n\nStep 11. Print only the occupation column\n\nprint(users['occupation'])\n\n0         technician\n1              other\n2             writer\n3         technician\n4              other\n           ...      \n938          student\n939    administrator\n940          student\n941        librarian\n942          student\nName: occupation, Length: 943, dtype: object\n\n\n\n\nStep 12. How many different occupations are in this dataset?\n\nusers.occupation.nunique()\n\n21\n\n\n\n\nStep 13. What is the most frequent occupation?\n\nmost_common_occupation = users['occupation'].value_counts().idxmax()\nprint(f\"{most_common_occupation}\")\n\nstudent\n\n\n\n\nStep 14. Summarize the DataFrame.\n\ndescription = users.describe(include='all')\nselected_columns = description.loc[:, [description.columns[0], 'age']]\nprint(selected_columns)\n\n           user_id         age\ncount   943.000000  943.000000\nunique         NaN         NaN\ntop            NaN         NaN\nfreq           NaN         NaN\nmean    472.000000   34.051962\nstd     272.364951   12.192740\nmin       1.000000    7.000000\n25%     236.500000   25.000000\n50%     472.000000   31.000000\n75%     707.500000   43.000000\nmax     943.000000   73.000000\n\n\n\n\nStep 15. Summarize all the columns\n\nsummary = users.describe(include='all').transpose()\npd.set_option('display.width', 400) \nprint(summary)\n\n            count unique      top freq       mean         std  min    25%    50%    75%    max\nuser_id     943.0    NaN      NaN  NaN      472.0  272.364951  1.0  236.5  472.0  707.5  943.0\nage         943.0    NaN      NaN  NaN  34.051962    12.19274  7.0   25.0   31.0   43.0   73.0\ngender        943      2        M  670        NaN         NaN  NaN    NaN    NaN    NaN    NaN\noccupation    943     21  student  196        NaN         NaN  NaN    NaN    NaN    NaN    NaN\nzip_code      943    795    55414    9        NaN         NaN  NaN    NaN    NaN    NaN    NaN\n\n\n\n\nStep 16. Summarize only the occupation column\n\noccupation_summary = users['occupation'].describe()\nprint(occupation_summary)\n\ncount         943\nunique         21\ntop       student\nfreq          196\nName: occupation, dtype: object\n\n\n\n\nStep 17. What is the mean age of users?\n\nimport numpy as np\nmean_age_floor = np.floor(users.age.mean()).astype(int)\nprint(mean_age_floor) \n\n34\n\n\n\n\nStep 18. What is the age with least occurrence?\n\nage_counts = users['age'].value_counts()\nleast_occurrence_ages = age_counts[age_counts == 1]\nleast_occurrence_ages.name = 'age'\nprint(least_occurrence_ages)\n\nage\n7     1\n11    1\n66    1\n10    1\n73    1\nName: age, dtype: int64",
    "crumbs": [
      "Exercises",
      "Lecture",
      "lab1-2 Occupation"
    ]
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Labs",
    "section": "",
    "text": "lab1-1 chipotle\n\n\nlab1-2 Occupation\n\n\nlab1-3 World Food Facts\n\n\nlab1-4 chipotle\n\n\nlab1-5 Euro12\n\n\nlab1-6 Chipotle\n\n\nlab1-7 Scores\n\n\nlab2"
  },
  {
    "objectID": "exercises/practice4.html",
    "href": "exercises/practice4.html",
    "title": "Tan Yan website",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\nimport seaborn as sns\n\n# The data is cleansed to ensure that the fields are the same in English\ndf_douban = pd.read_csv(\"data/douban_top250.csv\",encoding='gbk')\ndf_imdb = pd.read_csv(\"data/imdb_top_250_movies.csv\",encoding='gbk')\n\n\naverage_rating_douban = df_douban['rating'].mean()\naverage_rating_imdb = df_imdb['rating'].mean()\n\nprint(f\"Douban average rating: {average_rating_douban:.2f}\")\nprint(f\"IMDb average rating: {average_rating_imdb:.2f}\")\n\nplt.bar(['Douban', 'IMDb'], [average_rating_douban, average_rating_imdb], color=['blue', 'green'])\nplt.xlabel('platform')\nplt.ylabel('average rating')\nplt.title('Comparison of Average Scores between Douban and IMDb')\nplt.show()\n\nDouban average rating: 8.94\nIMDb average rating: 8.31\n\n\n\n\n\n\n\n\n\n\noverlap_movies = pd.merge(df_douban, df_imdb, on='title', how='inner')\nprint(\"The number of overlapping movies:\", len(overlap_movies))\n\nvenn2(subsets=(len(df_douban), len(df_imdb), len(overlap_movies)), set_labels=('Douban', 'IMDb'))\nplt.title('Overlapping situation between Douban and IMDb movies')\nplt.show()\n\nThe number of overlapping movies: 77\n\n\n\n\n\n\n\n\n\n\n\ncombined_df = pd.concat([df_douban, df_imdb], keys=['Douban', 'IMDb']).reset_index(level=0).rename(columns={'level_0': 'Source'})\n\nplt.figure(figsize=(12, 6))\nsns.histplot(data=combined_df, x='Year', hue='Source', bins=30, kde=True, multiple=\"stack\")\nplt.xlabel('Publication Year')\nplt.ylabel('Number of movies')\nplt.title('Number of movies')\nplt.legend(title='origin')\nplt.show()\n\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_1164\\2625814123.py:10: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  plt.legend(title='origin')\n\n\n\n\n\n\n\n\n\n\n#The director and style cannot be crawled at the moment because of the HTML structure",
    "crumbs": [
      "Exercises",
      "Practice 4 IMDb and Douban"
    ]
  },
  {
    "objectID": "exercises/practice2.html",
    "href": "exercises/practice2.html",
    "title": "2.1Collecting data by playing a public goods game",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='openpyxl')\nLetsPlot.setup_html(no_js=True)\n\n\nplt.style.use(\n\n    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n)\n\n\nQuestion 1\n\ndata = {\n    \"Copenhagen\": [14.5, 12.1, 12.5, 13.1, 11.3, 12.7, 11.8, 10.1, 9.4, 4.3],\n    \"Dniprop\": [11.0, 11.6, 12.1, 12.2, 10.3, 9.5, 6.5, 8.3, 7.1, 6.7],\n    \"Minsk\": [11.7, 13.3, 11.8, 11.4, 12.2, 9.6, 9.9, 7.4, 7.3, 6.2],\n}\ndf = pd.DataFrame.from_dict(data)\ndf.head()\n\n\n\n\n\n\n\n\nCopenhagen\nDniprop\nMinsk\n\n\n\n\n0\n14.5\n11.0\n11.7\n\n\n1\n12.1\n11.6\n13.3\n\n\n2\n12.5\n12.1\n11.8\n\n\n3\n13.1\n12.2\n11.4\n\n\n4\n11.3\n10.3\n12.2\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\ndf.plot(ax=ax)\nax.set_title(\"Average contributions to the public goods game: Without punishment\")\nax.set_ylabel(\"Average contribution\")\nax.set_xlabel(\"Round\");\n\n\n\n\n\n\n\n\n\n\nQuestion 2\nSimilarities: both depict changes in social contributions, demonstrate gradual change trends, and reflect changes in participants’ strategic adjustments over the course of the game # Question 3 Reason for similarity: human behaviour may show some stability and consistency in certain contexts. Reasons for differences: differences in the rules of the game, different characteristics of the group of participants, differences in the experimental environment, etc.\n\n\nPart 2.2 Describing the data\n\n\nQuestion 1（a）\n\ndata_np = pd.read_excel(\n    \"data/doing-economics-datafile-working-in-excel-project-2.xlsx\",\n    usecols=\"A:Q\",\n    header=1,\n    index_col=\"Period\",\n)\ndata_n = data_np.iloc[:10, :].copy()\ndata_p = data_np.iloc[14:24, :].copy()\n\nc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n  warn(msg)\n\n\n\ntest_data = {\n    \"City A\": [14.1, 14.1, 13.7],\n    \"City B\": [11.0, 12.6, 12.1],\n}\n\n\n# Original dataframe\ntest_df = pd.DataFrame.from_dict(test_data)\n# A copy of the dataframe\ntest_copy = test_df.copy()\n# A pointer to the dataframe\ntest_pointer = test_df\n\n\ntest_pointer.iloc[1, 1] = 99\n\n\nprint(\"test_df=\")\nprint(f\"{test_df}\\n\")\nprint(\"test_copy=\")\nprint(f\"{test_copy}\\n\")\n\ntest_df=\n   City A  City B\n0    14.1    11.0\n1    14.1    99.0\n2    13.7    12.1\n\ntest_copy=\n   City A  City B\n0    14.1    11.0\n1    14.1    12.6\n2    13.7    12.1\n\n\n\n\n\ndata_n.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 1 to 10\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   Copenhagen       10 non-null     object\n 1   Dnipropetrovs’k  10 non-null     object\n 2   Minsk            10 non-null     object\n 3   St. Gallen       10 non-null     object\n 4   Muscat           10 non-null     object\n 5   Samara           10 non-null     object\n 6   Zurich           10 non-null     object\n 7   Boston           10 non-null     object\n 8   Bonn             10 non-null     object\n 9   Chengdu          10 non-null     object\n 10  Seoul            10 non-null     object\n 11  Riyadh           10 non-null     object\n 12  Nottingham       10 non-null     object\n 13  Athens           10 non-null     object\n 14  Istanbul         10 non-null     object\n 15  Melbourne        10 non-null     object\ndtypes: object(16)\nmemory usage: 1.3+ KB\n\n\n\ndata_n = data_n.astype(\"double\")\ndata_p = data_p.astype(\"double\")\n\n\nmean_n_c = data_n.mean(axis=1)\nmean_p_c = data_p.agg(np.mean, axis=1)\n\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12316\\3801786469.py:2: FutureWarning: The provided callable &lt;function mean at 0x000001305E09E980&gt; is currently using DataFrame.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  mean_p_c = data_p.agg(np.mean, axis=1)\n\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n\n\nQuestion 1（b）\n\nfig, ax = plt.subplots()\nmean_n_c.plot(ax=ax, label=\"Without punishment\")\nmean_p_c.plot(ax=ax, label=\"With punishment\")\nax.set_title(\"Average contributions to the public goods game\")\nax.set_ylabel(\"Average contribution\")\nax.legend();\n\n\n\n\n\n\n\n\n\n\nQuestion 1（c）\nDifference: in the absence of penalties, the average contribution quickly reaches a relatively high peak early on, and with penalties, the average contribution is relatively low early on. Similar: the average contribution levels of the two experiments differed early on, but they both showed a trend over time\n\n\nQuestion 2\n\npartial_names_list = [\"F. Kennedy\", \"Lennon\", \"Maynard Keynes\", \"Wayne\"]\n[\"John \" + name for name in partial_names_list]\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n\n\n# Create new dataframe with bars in\ncompare_grps = pd.DataFrame(\n    [mean_n_c.loc[[1, 10]], mean_p_c.loc[[1, 10]]],\n    index=[\"Without punishment\", \"With punishment\"],\n)\n# Rename columns to have 'round' in them\ncompare_grps.columns = [\"Round \" + str(i) for i in compare_grps.columns]\n# Swap the column and index variables around with the transpose function, ready for plotting (.T is transpose)\ncompare_grps = compare_grps.T\n# Make a bar chart\ncompare_grps.plot.bar(rot=0);\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\nn_c = data_n.agg([\"std\", \"var\", \"mean\"], 1)\nn_c\n\n\n\n\n\n\n\n\nstd\nvar\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n1\n2.020724\n4.083325\n10.578313\n\n\n2\n2.238129\n5.009220\n10.628398\n\n\n3\n2.329569\n5.426891\n10.407079\n\n\n4\n2.068213\n4.277504\n9.813033\n\n\n5\n2.108329\n4.445049\n9.305433\n\n\n6\n2.240881\n5.021549\n8.454844\n\n\n7\n2.136614\n4.565117\n7.837568\n\n\n8\n2.349442\n5.519880\n7.376388\n\n\n9\n2.413845\n5.826645\n6.392985\n\n\n10\n2.187126\n4.783520\n4.383769\n\n\n\n\n\n\n\n\np_c = data_p.agg([\"std\", \"var\", \"mean\"], 1)\nfig, ax = plt.subplots()\nn_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 standard deviations\n(n_c[\"mean\"] + 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 standard deviations\n(n_c[\"mean\"] - 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_n.columns)):\n    ax.scatter(x=data_n.index, y=data_n.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game without punishment\")\nplt.show();\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\np_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 sd\n(p_c[\"mean\"] + 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 sd\n(p_c[\"mean\"] - 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_p.columns)):\n    ax.scatter(x=data_p.index, y=data_p.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game with punishment\")\nplt.show();\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\ndata_p.apply(lambda x: x.max() - x.min(), axis=1)\n\nPeriod\n1     10.199675\n2     12.185065\n3     12.689935\n4     12.625000\n5     12.140375\n6     12.827541\n7     13.098931\n8     13.482621\n9     13.496754\n10    11.307360\ndtype: float64\n\n\n\n# A lambda function accepting three inputs, a, b, and c, and calculating the sum of the squares\ntest_function = lambda a, b, c: a**2 + b**2 + c**2\n\n\n# Now we apply the function by handing over (in parenthesis) the following inputs: a=3, b=4 and c=5\ntest_function(3, 4, 5)\n\n50\n\n\n\nrange_function = lambda x: x.max() - x.min()\nrange_p = data_p.apply(range_function, axis=1)\nrange_n = data_n.apply(range_function, axis=1)\nfig, ax = plt.subplots()\nrange_p.plot(ax=ax, label=\"With punishment\")\nrange_n.plot(ax=ax, label=\"Without punishment\")\nax.set_ylim(0, None)\nax.legend()\nax.set_title(\"Range of contributions to the public goods game\")\nplt.show();\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5(a)\n\nfuncs_to_apply = [range_function, \"max\", \"min\", \"std\", \"mean\"]\nsumm_p = data_p.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\nsumm_n = data_n.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\n\n\nsumm_n.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n6.14\n14.10\n7.96\n2.02\n10.58\n\n\n10\n7.38\n8.68\n1.30\n2.19\n4.38\n\n\n\n\n\n\n\n\nsumm_p.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n10.20\n16.02\n5.82\n3.21\n10.64\n\n\n10\n11.31\n17.51\n6.20\n3.90\n12.87\n\n\n\n\n\n\n\n\n\nQuestion 5(b)\nSimilarities: in both experiments with and without penalties, the range of participants’ contributions changed over time, and in both experiments the range of contributions covered a certain range of values. Differences: in the experiment with punishment, the initial contribution range (Period 1) was higher at 12.5. in the experiment without punishment, the initial contribution range was lower at 10.0. in the experiment with punishment, the contribution ranges stabilised at a later stage, whereas in the experiment without punishment, the contribution ranges showed more volatility.\n\n\nPart 2.3 How did changing the rules of the game affect behaviour?\n\n\nQuestion 1(a)\n\n\ncoinsdata = {\n    \"the first time\": [2, 1, 2, 1,2, 2],\n    \"the second timep\": [2, 2,1, 2, 1,1],\n\n}\n\n\n\nQuestion 1(b)\nall different\n\n\nQuestion 2(a)\n\n# 2 (a)\n# Use the ttest function to calculate the p-value for the difference in means in Period 1 (with and without punishment).\n\nfrom scipy.stats import ttest_ind\n\nno_punishment_period_1 = data_np.iloc[0, :].astype(float)\npunishment_period_1 = data_np.iloc[14, :].astype(float)\n\nno_punishment_avg_period_1 = no_punishment_period_1.mean()\npunishment_avg_period_1 = punishment_period_1.mean()\n\nprint(f\"Contributions without punishment: {no_punishment_avg_period_1}\")\nprint(f\"Contributions with punishment: {punishment_avg_period_1}\")\n\nContributions without punishment: 10.57831343858623\nContributions with punishment: 10.638759195804596\n\n\n\nt_stat, p_value = ttest_ind(no_punishment_period_1, punishment_period_1)\n\n# p\nprint(f\"The p-value of the mean difference in the first period: {p_value}\")\n\nThe p-value of the mean difference in the first period: 0.9495666970741692\n\n\n\n\nQuestion 2(b)\nNo significant difference\n\n\nQuestion 3(a)\n\n# Use the ttest function to calculate the p-value for the difference in means in Period 10 (with and without punishment).\nno_punishment_period_10 = data_np.iloc[9, :].astype(float)\npunishment_period_10 = data_np.iloc[23, :].astype(float)\n\nno_punishment_avg_period_10 = no_punishment_period_10.mean()\npunishment_avg_period_10 = punishment_period_10.mean()\n\n# Calculate p-value using t-test\nt_stat_10, p_value_10 = ttest_ind(no_punishment_period_10, punishment_period_10)\n# p-value\nprint(f\"The p-value of the mean difference in the first period: {p_value_10}\")\n\nThe p-value of the mean difference in the first period: 1.809742363676441e-08\n\n\n\n\nQuestion 3(b)\nThe p-value data suggests a high degree of significance and that the punishment mechanism has a significant effect on the behaviour of individuals in the public goods game and that this effect is likely to increase the level of cooperation # Question 3(c) Figure 2.7 If the sample size is very small or the overall variance is very large, then even if the observed difference appears to be large. If the sample size is very large or the overall variance is very small, then even if the observed difference appears to be large, it may simply be a random fluctuation rather than a real overall difference, #Figure 8. Then even if the observed difference is small it may be highly statistically significant. So a judgement cannot be made on the basis of the size of the difference alone. A statistical significance test is performed to assess whether the observed differences are beyond what can be explained by random error.\n\n\nQuestion 4(a)\nAccording to the above analysis, the penalty rules are clear, the penalty is strong and immediately effective, and it can make people in the public goods game change their practices. # Question 4(b) To verify the causal relationship between punishment options and game behaviour, we need to compare the performance of the two groups with and without punishment in phase 1. By observing how the punishment mechanism affects participants’ contributory behaviour and quantifying the effect of this influence, we can more accurately understand the role of punishment in the game.\n\n\nQuestion 5\nLaboratory experiments have limitations in measuring social preferences, such as simplified environments, unrepresentative samples, overly controlled conditions and measurement errors. Solutions include enhancing experimental realism, expanding and diversifying samples, simulating realistic conditions, introducing random factors, and improving measurement precision to improve experimental accuracy and reliability and better understand the relationship between social preferences and real-life behaviours.",
    "crumbs": [
      "Exercises",
      "Practice2 empirical"
    ]
  },
  {
    "objectID": "exercises/index.html",
    "href": "exercises/index.html",
    "title": "Exercises",
    "section": "",
    "text": "Practice 1 Climate change\n\n\nPractice2 empirical\n\n\nPractice 3 Extract PDF and movie\n\n\nPractice 4 IMDb and Douban"
  },
  {
    "objectID": "exercises/Practice1.html",
    "href": "exercises/Practice1.html",
    "title": "Tan Yan website",
    "section": "",
    "text": "#The version used this time\n#Package            Version\n#------------------ ------------\n#python             3.13.0\n#ggplot             0.11.5\n#jupyter_client     8.6.3\n#jupyter_core       5.7.2\n#lets-plot          4.5.1\n#matplotlib         3.9.2\n#matplotlib-inline  0.1.7\n#numpy              2.1.2\n#pandas             2.2.3\n\n\nfrom matplotlib import pyplot as plt\nimport pandas as pd \nimport numpy as np\n\ndf = pd.read_csv(\n    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n    skiprows=1,\n    na_values=\"***\",\n)\ndf.head()\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145 entries, 0 to 144\nData columns (total 19 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    145 non-null    int64  \n 1   Jan     145 non-null    float64\n 2   Feb     145 non-null    float64\n 3   Mar     145 non-null    float64\n 4   Apr     145 non-null    float64\n 5   May     145 non-null    float64\n 6   Jun     145 non-null    float64\n 7   Jul     145 non-null    float64\n 8   Aug     145 non-null    float64\n 9   Sep     145 non-null    float64\n 10  Oct     144 non-null    float64\n 11  Nov     144 non-null    float64\n 12  Dec     144 non-null    float64\n 13  J-D     144 non-null    float64\n 14  D-N     143 non-null    float64\n 15  DJF     144 non-null    float64\n 16  MAM     145 non-null    float64\n 17  JJA     145 non-null    float64\n 18  SON     144 non-null    float64\ndtypes: float64(18), int64(1)\nmemory usage: 21.7 KB\n\n\n\ndf = df.set_index(\"Year\")\ndf.head()\ndf.tail()\nfig, ax = plt.subplots()\n# select the column to use 'plot' on, and pass the 'ax' object\n# note that the horizontal axis is given by the index of the dataframe\ndf[\"Jan\"].plot(ax=ax)\n# set the labels and title\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df.index, df[\"Jan\"])\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nmonth = \"Jan\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\");\n\n\n\n\n\n\n\n\n\n# Extra exercise 3\nyears = df.index.values\ndjf_data = df['DJF'].values\nmam_data = df['MAM'].values\njja_data = df['JJA'].values\nson_data = df['SON'].values\n \n# 折线图\nplt.figure(figsize=(12, 8))\n \n# DJF季节\nplt.subplot(2, 2, 1)\nplt.plot(years, djf_data, label='DJF')\nplt.title('Winter (DJF) Average Temperature Anomalies')\nplt.xlabel('Year')\nplt.ylabel('Temperature Anomaly')\nplt.grid(True)\nplt.legend()\n \n# MAM季节\nplt.subplot(2, 2, 2)\nplt.plot(years, mam_data, label='MAM')\nplt.title('Spring (MAM) Average Temperature Anomalies')\nplt.xlabel('Year')\nplt.ylabel('Temperature Anomaly')\nplt.grid(True)\nplt.legend()\n \n# JJA季节\nplt.subplot(2, 2, 3)\nplt.plot(years, jja_data, label='JJA')\nplt.title('Summer (JJA) Average Temperature Anomalies')\nplt.xlabel('Year')\nplt.ylabel('Temperature Anomaly')\nplt.grid(True)\nplt.legend()\n \n# SON季节\nplt.subplot(2, 2, 4)\nplt.plot(years, son_data, label='SON')\nplt.title('Autumn (SON) Average Temperature Anomalies')\nplt.xlabel('Year')\nplt.ylabel('Temperature Anomaly')\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Extra exercise 4\n\nannual_mean = df.mean(axis=1) \nplt.figure(figsize=(12, 6))\nplt.plot(annual_mean.index, annual_mean.values, label='Annual Average Temperature Anomaly')\nplt.ylabel('Annual Temperature Anomaly (°C)')\nplt.xlabel('Year')\nplt.title('Annual Average Temperature Anomaly in the Northern Hemisphere (1880-{})'.format(annual_mean.index.max()))\n \nmean_anomaly_1951_1980 = annual_mean.loc[1951:1980].mean()\n \nplt.axhline(y=mean_anomaly_1951_1980, color='red', linestyle='--', label='1951-1980 Average')\nplt.annotate(\n    '1951-1980 average',\n    xy=(annual_mean.index.min(), mean_anomaly_1951_1980 - 0.2), \n    xycoords=('data', 'data'),\n    xytext=(0.05, 0.9), \n    textcoords='axes fraction',\n    ha='left',\n    va='top',\n    fontsize=12,\n    color='red'\n)\n \nplt.legend()\n \nplt.grid(True)\n \nplt.show()\n\n\n\n\n\n\n\n\n\n#4（a）\nmonth = \"J-D\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.68, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average annual temperature anomaly in \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\");\n\n\n\n\n\n\n\n\n\n \n# 4(b): How do the graphs in questions 2 through 4(a) reflect the relationship between temperature and time?\n# Horizontal axis: in this graph, the horizontal axis represents time (years) from 1880 to the most recent year\n#Vertical axis: the vertical axis represents the average annual temperature anomaly (in degrees Celsius), which provides quantitative information about temperature change\n#Trends and Patterns: By looking at the line graphs, we can recognize if the trend of temperature anomalies is increasing or decreasing as well as seasonal variations or long-term trends\n#Horizontal line: The added horizontal line (representing the average temperature anomaly from 1951-1980) provides a reference point for comparing temperature anomalies across years\n\n#5. You now have graphs for three different time intervals: monthly (Question 2), seasonal (Question 3), and annual (Question 4). For each of these time intervals, discuss what we can learn about patterns in temperature over time from the graphs for the other time intervals that we might not otherwise learn about\n# (1) Months (Question 2)\n# Understandable: seasonal changes, monthly graphs can clearly show seasonal fluctuations in temperature, i.e. hot and cold months of the year, # (1) Monthly (Question 2)\n# Not Understandable: these data do not allow a direct look at long-term (e.g. decades) temperature trends.\n#(2) Seasons (Question 3)\n# Understandable: Seasonal charts can highlight differences in temperature between seasons.\n#Not understandable: seasonal charts do not provide detailed information about temperature changes within a given month\n#(3) Year (Q4)\n# Understandable: long-term trends can be seen and annual charts are best for identifying long-term temperature trends\n# Not understandable: annual charts do not capture temperature changes over monthly or shorter time intervals\n#6. Compare the graph in Question 4 with Figure 1.5, which also shows temperature changes over time using data from the National Academy of Sciences.\n# (1) Discuss similarities and differences between the graphs. (For example, are the horizontal and vertical axis variables the same, or are the lines the same shape?)\n# Similarities: Both graphs have a horizontal axis that represents the year, a vertical axis that represents the temperature in degrees Celsius, and both use line graphs to depict temperature trends over time.\n#Differences: graph 1.4 covers the period from 1880 to 2024, while image 1.5 covers the period from 1000 to 2000, with scales of 20 and 100 years. The vertical scale Figure 1.4 has a temperature change of 0.5 and Figure 1.5 has a temperature change of 0.2.\n#(2) Looking at the temperature change over time from 1000 to 1900 in Figure 1.4, is the pattern observed in the graph unusual?\n#The change in temperature over time from 1000 to 1900, the red line has a significant upward trend in 1900 and 2000, which to some extent may indicate a warming trend .\n#(3) Based on your answers to questions 4 and 5, do you think the government should be concerned about climate change?\n#From the comparison of the above data, it can be seen that the global climate is now accelerating warming and the temperature is rising, so the government should pay attention to the climate change and deal with global warming. It should develop mitigation measures such as reducing greenhouse gas emissions and developing renewable energy to slow down the rate of global warming.\n\n\n\n#Part 1.2 Variation in temperature over time\n\n\n#Question 1\ndf[\"Period\"] = pd.cut(\n    df.index,\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\ndf[\"Period\"].tail(20)\n\nYear\n2005    1981—2010\n2006    1981—2010\n2007    1981—2010\n2008    1981—2010\n2009    1981—2010\n2010    1981—2010\n2011          NaN\n2012          NaN\n2013          NaN\n2014          NaN\n2015          NaN\n2016          NaN\n2017          NaN\n2018          NaN\n2019          NaN\n2020          NaN\n2021          NaN\n2022          NaN\n2023          NaN\n2024          NaN\nName: Period, dtype: category\nCategories (3, object): ['1921—1950' &lt; '1951—1980' &lt; '1981—2010']\n\n\n\nlist_of_months = [\"Jun\", \"Jul\", \"Aug\"]\ndf[list_of_months].stack().head()\n\nYear     \n1880  Jun   -0.18\n      Jul   -0.22\n      Aug   -0.26\n1881  Jun   -0.34\n      Jul    0.09\ndtype: float64\n\n\n\n#Question 2（a）\nfig, axes = plt.subplots(ncols=3, figsize=(9, 4), sharex=True, sharey=True)\nfor ax, period in zip(axes, df[\"Period\"].dropna().unique()):\n    df.loc[df[\"Period\"] == period, list_of_months].stack().hist(ax=ax)\n    ax.set_title(period)\nplt.suptitle(\"Histogram of temperature anomalies\")\naxes[1].set_xlabel(\"Summer temperature distribution\")\nplt.tight_layout();\n\n#Question 2（b）\n#（1）similarities：The distribution of temperature anomalies shows a gradual increase from left to right in both time periods\n#（2）differences：The overall height of the histogram is higher in 1981-2010 than in 1951-1980, and the frequency of temperature anomalies increases during 1981-2010.\n\n\n\n\n\n\n\n\n\n\n#Question 3\ntemp_all_months = df.loc[(df.index &gt;= 1951) & (df.index &lt;= 1980), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at this data:\ntemp_all_months\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1951\nJan\n-0.36\n\n\n1\n1951\nFeb\n-0.51\n\n\n2\n1951\nMar\n-0.19\n\n\n3\n1951\nApr\n0.07\n\n\n4\n1951\nMay\n0.17\n\n\n...\n...\n...\n...\n\n\n355\n1980\nAug\n0.09\n\n\n356\n1980\nSep\n0.10\n\n\n357\n1980\nOct\n0.12\n\n\n358\n1980\nNov\n0.20\n\n\n359\n1980\nDec\n0.09\n\n\n\n\n360 rows × 3 columns\n\n\n\n\nquantiles = [0.3, 0.7]\nlist_of_percentiles = np.quantile(temp_all_months[\"values\"], q=quantiles)\n\nprint(f\"The cold threshold of {quantiles[0]*100}% is {list_of_percentiles[0]}\")\nprint(f\"The hot threshold of {quantiles[1]*100}% is {list_of_percentiles[1]}\")\n\nThe cold threshold of 30.0% is -0.1\nThe hot threshold of 70.0% is 0.1\n\n\n\n#Question 4\ntemp_all_months = df.loc[(df.index &gt;= 1981) & (df.index &lt;= 2010), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at the start of this data data:\ntemp_all_months.head()\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1981\nJan\n0.79\n\n\n1\n1981\nFeb\n0.62\n\n\n2\n1981\nMar\n0.68\n\n\n3\n1981\nApr\n0.39\n\n\n4\n1981\nMay\n0.18\n\n\n\n\n\n\n\n\nentries_less_than_q30 = temp_all_months[\"values\"] &lt; list_of_percentiles[0]\nproportion_under_q30 = entries_less_than_q30.mean()\nprint(\n    f\"The proportion under {list_of_percentiles[0]} is {proportion_under_q30*100:.2f}%\"\n)\n\nThe proportion under -0.1 is 1.94%\n\n\n\nproportion_over_q70 = (temp_all_months[\"values\"] &gt; list_of_percentiles[1]).mean()\nprint(f\"The proportion over {list_of_percentiles[1]} is {proportion_over_q70*100:.2f}%\")\n\nThe proportion over 0.1 is 84.72%\n\n\n\n#Question 5（a）\ntemp_all_months = (\n    df.loc[:, \"DJF\":\"SON\"]\n    .stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"Season\", 0: \"Values\"})\n)\ntemp_all_months[\"Period\"] = pd.cut(\n    temp_all_months[\"Year\"],\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n# Take a look at a cut of the data using `.iloc`, which provides position\ntemp_all_months.iloc[-135:-125]\n\n\n\n\n\n\n\n\nYear\nSeason\nValues\nPeriod\n\n\n\n\n443\n1991\nDJF\n0.51\n1981—2010\n\n\n444\n1991\nMAM\n0.45\n1981—2010\n\n\n445\n1991\nJJA\n0.42\n1981—2010\n\n\n446\n1991\nSON\n0.32\n1981—2010\n\n\n447\n1992\nDJF\n0.43\n1981—2010\n\n\n448\n1992\nMAM\n0.30\n1981—2010\n\n\n449\n1992\nJJA\n-0.04\n1981—2010\n\n\n450\n1992\nSON\n-0.15\n1981—2010\n\n\n451\n1993\nDJF\n0.37\n1981—2010\n\n\n452\n1993\nMAM\n0.31\n1981—2010\n\n\n\n\n\n\n\n\ngrp_mean_var = temp_all_months.groupby([\"Season\", \"Period\"])[\"Values\"].agg(\n    [\"mean\", \"var\"]\n)\ngrp_mean_var\n\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_12184\\1296543822.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grp_mean_var = temp_all_months.groupby([\"Season\", \"Period\"])[\"Values\"].agg(\n\n\n\n\n\n\n\n\n\n\nmean\nvar\n\n\nSeason\nPeriod\n\n\n\n\n\n\nDJF\n1921—1950\n-0.027931\n0.057703\n\n\n1951—1980\n-0.003333\n0.050375\n\n\n1981—2010\n0.522000\n0.078644\n\n\nJJA\n1921—1950\n-0.054483\n0.021611\n\n\n1951—1980\n0.001333\n0.014640\n\n\n1981—2010\n0.399000\n0.067775\n\n\nMAM\n1921—1950\n-0.041724\n0.031136\n\n\n1951—1980\n0.000333\n0.025272\n\n\n1981—2010\n0.507667\n0.075812\n\n\nSON\n1921—1950\n0.081379\n0.027798\n\n\n1951—1980\n-0.001333\n0.026384\n\n\n1981—2010\n0.427000\n0.110739\n\n\n\n\n\n\n\n\n#Question 5（b）\nmin_year = 1880\n# Because the sample code ggplot with my current version of pandas is not compatible, the following use of matplotlib.pyplot function\nplt.figure(figsize=(10, 6))\n \nseason_colors = {\n    'DJF': 'purple',\n    'JJA': 'blue',\n    'MAM': 'red',\n    'SON': 'green'\n} \n \n\nfor season, group in temp_all_months.groupby('Season'):\n    color = season_colors.get(season, 'black')  \n    plt.plot(group['Year'], group['Values'], label=season, color=color)\n \nplt.axhline(y=0, color='black', linewidth=1)\n\nplt.title(f\"Average annual temperature anomaly in \\n the northern hemisphere ({min_year}—{temp_all_months['Year'].max()})\")\nplt.ylabel(\"Annual temperature anomalies\")\nplt.xlabel(\"Year\")\n \nplt.xticks(rotation=45)\n \nplt.text(min_year, 0.1, '1951—1980 average', horizontalalignment='left', color='black')\n\nplt.legend()\n \n# 显示图表\nplt.grid(True)\nplt.tight_layout() \nplt.show()\n\n\n\n\n\n\n\n\n\n#Part 1.3 Carbon emissions and the environment\n\n\n#Question 1\n#The relatively remote and elevated location of Mauna Loa Observatory on a mountaintop on the Big Island of Hawaii helps to minimize the impact of localized pollution on the observational data, which are analyzed and interpreted by authorities such as the Earth System Research Laboratory (ESRL), often taking into account a variety of factors, including meteorological conditions, atmospheric circulation patterns, and anthropogenic impacts on atmospheric constituents, which are of significant informational value as described above.\n#Question 2\n#The difference between trend and interpolated is mainly due to their different emphasis. trend focuses more on long-term, stable trends, whereas interpolated focuses more on data completeness and continuity, and seasonal variations in CO2 levels are affected by a variety of factors, which together determine the spatial and temporal distributions of atmospheric CO2 concentrations.\n\n\n#Question 3\n#CO2 is proportional to time.\ndata = pd.read_csv('data/1_CO2-data.csv')\n \ndata['Month'] = data['Month'].astype(str).str.zfill(2) \ndata['Date'] = data.apply(lambda row: f\"{row['Year']}-{row['Month']}-01\", axis=1)\n \ndata['Date'] = pd.to_datetime(data['Date'])\ndata.set_index('Date', inplace=True)\n\n\n\nfig, ax = plt.subplots(figsize=(12, 6))\n \n# 插值CO2数据\nax.plot(data.index, data['Interpolated'], label='Interpolated CO2', color='blue', marker='o')\n \n# 趋势CO2数据\nax.plot(data.index, data['Trend'], label='Trend CO2', color='red', linestyle='--')\n \nax.set_title('Interpolated and Trend CO2 Levels from January 1960')\nax.set_xlabel('Date')\nax.set_ylabel('CO2 Level (ppm)')\n \nplt.legend()\nplt.grid(True)\n \nplt.show()\n\n\n\n\n\n\n\n\n\n#Question 4（a）\ndf_co2 = pd.read_csv(\"data/1_CO2-data.csv\")\ndf_co2.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n0\n1958\n3\n315.71\n315.71\n314.62\n\n\n1\n1958\n4\n317.45\n317.45\n315.29\n\n\n2\n1958\n5\n317.50\n317.50\n314.71\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n4\n1958\n7\n315.86\n315.86\n314.98\n\n\n\n\n\n\n\n\ndf_co2_june = df_co2.loc[df_co2[\"Month\"] == 6]\ndf_co2_june.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n15\n1959\n6\n318.15\n318.15\n315.92\n\n\n27\n1960\n6\n319.59\n319.59\n317.36\n\n\n39\n1961\n6\n319.77\n319.77\n317.48\n\n\n51\n1962\n6\n320.55\n320.55\n318.27\n\n\n\n\n\n\n\n\ndf_temp_co2 = pd.merge(df_co2_june, df, on=\"Year\")\ndf_temp_co2[[\"Year\", \"Jun\", \"Trend\"]].head()\n\n\n\n\n\n\n\n\nYear\nJun\nTrend\n\n\n\n\n0\n1958\n0.05\n314.85\n\n\n1\n1959\n0.14\n315.92\n\n\n2\n1960\n0.18\n317.36\n\n\n3\n1961\n0.18\n317.48\n\n\n4\n1962\n-0.13\n318.27\n\n\n\n\n\n\n\n\n# Because the sample code ggplot with my current version of pandas is not compatible, the following use of matplotlib.pyplot function\nplt.figure(figsize=(10, 6))\n \nplt.scatter(df_temp_co2['Jun'], df_temp_co2['Trend'], color='black', s=50)  \n \nplt.title(\"Scatterplot of temperature anomalies vs carbon dioxide emissions\")\nplt.ylabel(\"Carbon dioxide levels (trend, mole fraction)\")\nplt.xlabel(\"Temperature anomaly (degrees Celsius)\")\n \nplt.grid(True) \nplt.show()\n\n\n\n\n\n\n\n\n\n#Question 4（b）\ndf_temp_co2[[\"Jun\", \"Trend\"]].corr(method=\"pearson\")\n\n#Question 4（c）：The Pearson correlation coefficient only indicates linear correlation between variables and does not imply causation. Even if there is a high correlation between two variables, it cannot be directly inferred that there is a causal relationship between them\n\n\n\n\n\n\n\n\nJun\nTrend\n\n\n\n\nJun\n1.000000\n0.914371\n\n\nTrend\n0.914371\n1.000000\n\n\n\n\n\n\n\n\n# Because the sample code ggplot with my current version of pandas is not compatible, the following use of matplotlib.pyplot function\nplt.figure(figsize=(10, 6))\n \nplt.plot(df_temp_co2['Year'], df_temp_co2['Jun'], linewidth=1)\n \nplt.title(\"June temperature anomalies\")\nplt.xticks(rotation=45) \n \nplt.xlabel(\"Year\")\nplt.ylabel(\"June temperature anomalies\")\nplt.grid(True) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Because the sample code ggplot with my current version of pandas is not compatible, the following use of matplotlib.pyplot function\ndef plot_data(df, x_col, y_col, title, ax):\n    ax.plot(df[x_col], df[y_col], linewidth=1)\n    \n    ax.set_title(title)\n    ax.set_xlabel(x_col)\n    ax.set_ylabel(y_col.replace('_', ' ').capitalize())\n    \n    ax.grid(True)\n \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n \nplot_data(df_temp_co2, 'Year', 'Jun', 'June Temperature Anomalies', ax1)\n \nplot_data(df_temp_co2, 'Year', 'Trend', 'Carbon Dioxide Emissions', ax2)\n \nplt.tight_layout()\n \nplt.show()\n\n\n\n\n\n\n\n\n\n# Extra exercise 5\ndf_co2 = pd.read_csv(\"data/1_CO2-data.csv\")\n\ndf = pd.read_csv(\n    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n    skiprows=1,\n    na_values=\"***\",\n)\n \ndf_co2_jan = df_co2.loc[df_co2[\"Month\"] == 1, [\"Year\", \"Trend\"]].rename(columns={\"Trend\": \"CO2_Jan\"})\ndf_co2_jul = df_co2.loc[df_co2[\"Month\"] == 7, [\"Year\", \"Trend\"]].rename(columns={\"Trend\": \"CO2_Jul\"})\ndf.reset_index(inplace=True)\ndf_temp_co2_jan = pd.merge(df_co2_jan, df[[\"Year\", \"Jan\"]], on=\"Year\")\n \ndf_temp_co2_jul = pd.merge(df_co2_jul, df[[\"Year\", \"Jul\"]], on=\"Year\")\n\ncorrelation_jan = df_temp_co2_jan['Jan'].corr(df_temp_co2_jan['CO2_Jan'])\ncorrelation_jul = df_temp_co2_jul['Jul'].corr(df_temp_co2_jul['CO2_Jul'])\n \nprint(f\"January Pearson: {correlation_jan}\")\nprint(f\"July Pearson: {correlation_jul}\")\n \n# 绘制散点图\nplt.figure(figsize=(14, 6))\n \n# 一月散点图\nplt.subplot(1, 2, 1)\nplt.scatter(df_temp_co2_jan['Jan'], df_temp_co2_jan['CO2_Jan'], color='black', s=50)\nplt.title(\"January Temperature Anomalies and Carbon Dioxide Levels\")\nplt.ylabel(\"Carbon dioxide levels (ppm)\")\nplt.xlabel(\"temperature anomaly \")\nplt.grid(True)\n \n# 七月散点图\nplt.subplot(1, 2, 2)\nplt.scatter(df_temp_co2_jul['Jul'], df_temp_co2_jul['CO2_Jul'], color='red', s=50)\nplt.title(\"July Temperature Anomalies and Carbon Dioxide Levels\")\nplt.ylabel(\"Carbon dioxide levels (ppm)\")\nplt.xlabel(\"temperature anomaly \")\nplt.grid(True)\n \nplt.tight_layout()\nplt.show()\n\n#QA：The graph visualizes the distribution of data between temperature anomalies and CO₂ levels for January and July in the form of a scatter plot.\n# It can be seen from the graph that as the temperature anomaly increases, the CO₂ level shows a corresponding increasing trend, \n# and there is a significant linear positive correlation between the CO₂ level and the temperature anomaly\n\nJanuary Pearson: 0.8280347102335861\nJuly Pearson: 0.9096960952776127\n\n\n\n\n\n\n\n\n\n\n#Question 6（a）\n#Although correlation and causation are both concepts that describe the relationship between variables\n#but they are different in nature,Correlation only reveals the association between variables\n# while causation points out the causal link between variables\n#In practice, we need to analyze the data carefully to avoid being misled by false correlations and try to determine the true relationship between variables\n\n#Question 6（b）\n#In analyzing the data, a negative correlation was found between urban green coverage and carbon dioxide levels\n#and it was concluded that increasing green coverage was an effective means of reducing carbon dioxide levels.\n#The result was that areas with high urban green coverage tended to be residential, commercial or park areas with less human activity\n#while areas with higher carbon dioxide levels tended to be industrial or heavily trafficked areas.\n#Therefore, it is the intensity and type of human activity,\n# rather than the percentage of green cover, that is the real cause of the difference in CO2 levels\n\n#Question 6（c）\n#I think the correlation in this example is less likely to be coincidental and more likely to be a spurious correlation caused by one or more other variables. \n#The correlation is statistically significant but not causally related in substance, and both may be influenced by some economic, social, or cultural trend.\n# For example, people may be more inclined to engage in recreational activities (e.g., searching for information about “batman”) during a boom\n#and the security industry may expand as a result of increased business activity. Conversely, these trends may be reversed during economic downturns.",
    "crumbs": [
      "Exercises",
      "Practice 1 Climate change"
    ]
  },
  {
    "objectID": "exercises/practice3.html",
    "href": "exercises/practice3.html",
    "title": "1. Go through https://aeturrell.github.io/coding-for-economists/data-extraction.html up to ‘Extracting data from PDFs’ section. Recreate the code on your machine.",
    "section": "",
    "text": "import fitz #Couldn't be read with the pdftotext function because of the version, so it's replaced with the PyMuPDF\nfrom pathlib import Path\n \n\npdf_path = Path(\"data/pdf_with_table.pdf\")  \n\ntry:\n    pdf_document = fitz.open(pdf_path)  \n    all_text = \"\" \n \n    for page_num in range(pdf_document.page_count):\n        page = pdf_document.load_page(page_num) \n        page_text = page.get_text()  \n        all_text += page_text + \"\\n\\n\"  \n    \n    print(all_text[:220])  \n \nfinally:\n\n    pdf_document.close() \n \n\n3 \n \n2 Quantifying Fuel-Saving Opportunities from Specific Driving \nBehavior Changes \n2.1 \nSavings from Improving Individual Driving Profiles \n2.1.1 Drive Profile Subsample from Real-World Travel Survey \nThe interim repo\n\n\n\nimport pdfplumber #Changed functions due to system version\n \nwith pdfplumber.open('data/pdf_with_table.pdf') as pdf:\n\n    for page in pdf.pages:   \n        tables = page.extract_tables()\n        table = tables[0]\n        for row in table:\n            print(row)\n\n['Cycle\\nName', 'KI\\n(1/km)', 'Distance\\n(mi)', 'Percent Fuel Savings', None, None, None]\n[None, None, None, 'Improved\\nSpeed', 'Decreased\\nAccel', 'Eliminate\\nStops', 'Decreased\\nIdle']\n['2012_2', '3.30', '1.3', '5.9%', '9.5%', '29.2%', '17.4%']\n['2145_1', '0.68', '11.2', '2.4%', '0.1%', '9.5%', '2.7%']\n['4234_1', '0.59', '58.7', '8.5%', '1.3%', '8.5%', '3.3%']\n['2032_2', '0.17', '57.8', '21.7%', '0.3%', '2.7%', '1.2%']\n['4171_1', '0.07', '173.9', '58.1%', '1.6%', '2.1%', '0.5%']\n\n\n\n#import textract\n#text = textract.process(Path('path/to/file.extension'))\n#Currently, this third-party library does not run on Windows.\n\n\n2.Go through https://www.geeksforgeeks.org/scrape-imdb-movie-rating-and-detalls-using-python/. Recreate the code on your machine.\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nfrom time import sleep\n\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/116.0',\n    'Accept': '*/*',\n    'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',\n    'Accept-Encoding': 'gzip, deflate, br',\n    'Content-Type': 'text/plain;charset=UTF-8',\n}\n\nurl = 'https://www.imdb.com/chart/top'\nresponse = requests.get(url, headers=headers)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nmovies = soup.select('li.ipc-metadata-list-summary-item')\n\n\nindex = 0\n\ntitle_string = movies[index].select('h3')[0].text.split(\". \")\nplace = title_string[0]\nmovie_title = title_string[1]\nrating = movies[index].select('span.ipc-rating-star--rating')[0].text\nyear = movies[index].select('span.cli-title-metadata-item')[0].text\ntitle_id_href = movies[0].select('a.ipc-lockup-overlay')[0].get('href')\ntitle_id = title_id_href.split(\"/\")[2]\nstats_url = 'https://caching.graphql.imdb.com/?operationName=Title_Summary_Prompt_From_Base&variables={\"id\":\"' + title_id + '\",\"locale\":\"zh-CN\",\"location\":{\"latLong\":{\"lat\":\"22.3\",\"long\":\"114.2\"}},\"promotedProvider\":null,\"providerId\":null}&extensions={\"persistedQuery\":{\"sha256Hash\":\"28fdb27482f0852bc70811f36a5fab72e9afd1589b37c5e1aaecf438d418f6c7\",\"version\":1}}'\nstats_response = requests.get(stats_url, headers=headers)\nstats_json_data = stats_response.json()\nstats = ', '.join([item['name']['nameText']['text'] for item in stats_json_data['data']['title']['principalCast'][0]['credits']])\nstats_json_data\n\n\nmovie_data_list = []\n\nfor index in range(len(movies)):\n    title_string = movies[index].select('h3')[0].text.split(\". \")\n    place = title_string[0]\n    movie_title = title_string[1]\n    rating = movies[index].select('span.ipc-rating-star--rating')[0].text\n    year = movies[index].select('span.cli-title-metadata-item')[0].text\n    title_id_href = movies[index].select('a.ipc-lockup-overlay')[0].get('href')\n    title_id = title_id_href.split(\"/\")[2]\n    stats_url = 'https://caching.graphql.imdb.com/?operationName=Title_Summary_Prompt_From_Base&variables={\"id\":\"' + title_id + '\",\"locale\":\"zh-CN\",\"location\":{\"latLong\":{\"lat\":\"22.3\",\"long\":\"114.2\"}},\"promotedProvider\":null,\"providerId\":null}&extensions={\"persistedQuery\":{\"sha256Hash\":\"28fdb27482f0852bc70811f36a5fab72e9afd1589b37c5e1aaecf438d418f6c7\",\"version\":1}}'\n    stats_response = requests.get(stats_url, headers=headers)\n    stats_json_data = stats_response.json()\n    stats = ', '.join([item['name']['nameText']['text'] for item in stats_json_data['data']['title']['principalCast'][0]['credits']])\n\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": rating,\n            \"year\": year,\n            \"star_cast\": stats,\n            }\n    movie_data_list.append(data)\n    sleep(0.2)\n\n\ndf = pd.DataFrame(movie_data_list)\ndf.to_csv('data/imdb_top_250_movies.csv',index=False)\n\n\ndf\n\n\n\n3.scrape data from hitps:/movie.douban.com/top250 and make visualizations.Use hitps:/blog,csdn.net/9g 5174909/article/detais/143106786 as an example\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n \ndouban_url_template = \"https://movie.douban.com/top250?start={}\"\ndouban_headers = {\n    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/116.0',\n}\n \ndouban_movies = []\n\n\n# 分页\nfor start in range(0, 250, 25):\n    douban_url = douban_url_template.format(start)\n    douban_response = requests.get(douban_url, headers=douban_headers)\n    douban_response.encoding = 'utf-8'  # 设置编码方式\n    douban_soup = BeautifulSoup(douban_response.text, 'html.parser')\n \n    for item in douban_soup.find_all('div', class_='item'):\n        title = item.find('span', class_='title').get_text()  # 电影名称\n        \n        # 获取年份信息\n        year_info = item.find('p').find('span', class_='pl')\n        if year_info:\n            year = year_info.get_text().strip().replace('(', '').replace(')', '')  # 清理括号\n        else:\n            year = '未知'\n \n        description_span = item.find('span', class_='inq')  # 电影描述\n        rating = item.find('span', class_='rating_num').get_text()  # 评分\n        \n        try:\n            votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数\n        except IndexError:\n            # 如果找不到，可以尝试其他方式或者设置为None/默认值\n            votes = '未知'\n        \n        # 如果没有描述，将其置为空字符串\n        description = description_span.get_text() if description_span else ''\n        \n        movie = {\n            \"title\": title,\n            \"year\": year,\n            \"description\": description,\n            \"rating\": rating,\n            \"votes\": votes.replace('人评价', '').strip() if votes else '0'  # 如果没有votes，则设置为0或其他默认值\n        }\n        douban_movies.append(movie)\n \n\n\ndouban_df = pd.DataFrame(douban_movies)\ndouban_df.to_csv('data/douban_top250.csv', index=False)\n\n\ndouban_df",
    "crumbs": [
      "Exercises",
      "Practice 3 Extract PDF and movie"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TAN YAN",
    "section": "",
    "text": "Github\n  \n\n\n\n\nmy website!\n\n~BSU，EM24-A，TAN YAN\n\n~English major, loves swimming"
  },
  {
    "objectID": "labs/lab1-1 chipotle .html",
    "href": "labs/lab1-1 chipotle .html",
    "title": "Ex2 - Getting and Knowing your Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport requests\n \n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\ndf = pd.read_csv(url,sep='\\t')\nchipo = df\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. What is the number of observations in the dataset?\n\n# Solution 1\nnum_rows, num_columns = df.shape\n \nprint(f\"{num_rows}\")\n\n4622\n\n\n\n# Solution 2\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4622 entries, 0 to 4621\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   order_id            4622 non-null   int64 \n 1   quantity            4622 non-null   int64 \n 2   item_name           4622 non-null   object\n 3   choice_description  3376 non-null   object\n 4   item_price          4622 non-null   object\ndtypes: int64(2), object(3)\nmemory usage: 180.7+ KB\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\nnum_columns = df.shape[1]\n    \n    # 打印列数\nprint(f\"{num_columns}\")\n\n5\n\n\n\n\nStep 7. Print the name of all the columns.\n\ncolumn_names = df.columns.tolist()\ncolumn_names_str = \", \".join(column_names)\n \nindex_str = f\"Index([{', '.join([f'u\\'{col}\\'' for col in column_names])}],\\n       dtype='object')\"\nprint(index_str)\n\nIndex([u'order_id', u'quantity', u'item_name', u'choice_description', u'item_price'],\n       dtype='object')\n\n\n\n\nStep 8. How is the dataset indexed?\n\nindex = df.index\nif isinstance(index, pd.RangeIndex):\n    print(f\"RangeIndex(start={index.start}, stop={index.stop}, step={index.step})\")\n\nRangeIndex(start=0, stop=4622, step=1)\n\n\n\n\nStep 9. Which was the most-ordered item?\n\nitem_quants = chipo.groupby(['item_name']).agg({'quantity':'sum'})\nitem_quants.sort_values('quantity', ascending=False)\n\n\n\n\n\n\n\n\nquantity\n\n\nitem_name\n\n\n\n\n\nChicken Bowl\n761\n\n\nChicken Burrito\n591\n\n\nChips and Guacamole\n506\n\n\nSteak Burrito\n386\n\n\nCanned Soft Drink\n351\n\n\nChips\n230\n\n\nSteak Bowl\n221\n\n\nBottled Water\n211\n\n\nChips and Fresh Tomato Salsa\n130\n\n\nCanned Soda\n126\n\n\nChicken Salad Bowl\n123\n\n\nChicken Soft Tacos\n120\n\n\nSide of Chips\n110\n\n\nVeggie Burrito\n97\n\n\nBarbacoa Burrito\n91\n\n\nVeggie Bowl\n87\n\n\nCarnitas Bowl\n71\n\n\nBarbacoa Bowl\n66\n\n\nCarnitas Burrito\n60\n\n\nSteak Soft Tacos\n56\n\n\n6 Pack Soft Drink\n55\n\n\nChicken Crispy Tacos\n50\n\n\nChips and Tomatillo Red Chili Salsa\n50\n\n\nChips and Tomatillo Green Chili Salsa\n45\n\n\nCarnitas Soft Tacos\n40\n\n\nSteak Crispy Tacos\n36\n\n\nChips and Tomatillo-Green Chili Salsa\n33\n\n\nSteak Salad Bowl\n31\n\n\nNantucket Nectar\n29\n\n\nChips and Tomatillo-Red Chili Salsa\n25\n\n\nBarbacoa Soft Tacos\n25\n\n\nChips and Roasted Chili Corn Salsa\n23\n\n\nIzze\n20\n\n\nVeggie Salad Bowl\n18\n\n\nChips and Roasted Chili-Corn Salsa\n18\n\n\nBarbacoa Crispy Tacos\n12\n\n\nBarbacoa Salad Bowl\n10\n\n\nChicken Salad\n9\n\n\nCarnitas Crispy Tacos\n8\n\n\nVeggie Soft Tacos\n8\n\n\nBurrito\n6\n\n\nCarnitas Salad Bowl\n6\n\n\nVeggie Salad\n6\n\n\nBowl\n4\n\n\nSteak Salad\n4\n\n\nSalad\n2\n\n\nCrispy Tacos\n2\n\n\nCarnitas Salad\n1\n\n\nChips and Mild Fresh Tomato Salsa\n1\n\n\nVeggie Crispy Tacos\n1\n\n\n\n\n\n\n\n\n\nStep 10. For the most-ordered item, how many items were ordered?\n\nitem_quants.sort_values('quantity', ascending=False).iloc[0]\n\nquantity    761\nName: Chicken Bowl, dtype: int64\n\n\n\n\nStep 11. What was the most ordered item in the choice_description column?\n\nitem_quants = chipo.groupby(['choice_description']).agg({'quantity':'sum'})\nitem_quants.sort_values('quantity',ascending=False)[:5]\n\n\n\n\n\n\n\n\nquantity\n\n\nchoice_description\n\n\n\n\n\n[Diet Coke]\n159\n\n\n[Coke]\n143\n\n\n[Sprite]\n89\n\n\n[Fresh Tomato Salsa, [Rice, Black Beans, Cheese, Sour Cream, Lettuce]]\n49\n\n\n[Fresh Tomato Salsa, [Rice, Black Beans, Cheese, Sour Cream]]\n42\n\n\n\n\n\n\n\n\n\nStep 12. How many items were orderd in total?\n\ntotal_quantity = df['quantity'].sum()\nprint(f\"{total_quantity} \")\n\n4972 \n\n\n\n\nStep 13. Turn the item price into a float\n\nStep 13.a. Check the item price type\n\n\nchipo['item_price'] = chipo['item_price']\nprint(chipo['item_price'].dtype)\n\nfloat64\n\n\n\n\nStep 13.b. Create a lambda function and change the type of item price\n\ndf['item_price'] = df['item_price'].apply(lambda x: pd.to_numeric(x, errors='coerce')).astype('float64')\n    \nselected_columns = df[['order_id', 'quantity', 'item_name', 'choice_description', 'item_price']].head(10)\nprice_dtype = selected_columns['item_price'].dtype\n\n\n\n\nStep 13.c. Check the item price type\n\n\nprint(f\"dtype('{price_dtype}')\")\n\ndtype('float64')\n\n\n\n\n\nStep 14. How much was the revenue for the period in the dataset?\n\nchipo = pd.read_csv(url, sep='\\t')\nchipo['item_price'] = chipo['item_price'].apply(lambda x: float(x.replace('$', '')))\ntotal_revenue = chipo['item_price'].sum()\nformatted_revenue = \"${:,.2f}\".format(total_revenue)\nprint(f\"Revenue was: {formatted_revenue}\")\n\nRevenue was: $34,500.16\n\n\n\n\nStep 15. How many orders were made in the period?\n\nunique_order_count = df['order_id'].nunique()\n\nprint(f'{unique_order_count} ')\n\n1834 \n\n\n\n\nStep 16. What is the average revenue amount per order?\n\n# Solution 1\naverage_price = chipo['item_price'].mean()\nformatted_average_price = \"${:,.2f}\".format(average_price)\n\n\n# Solution 2\nprint(f\"Average Item Price was: {formatted_average_price}\")\n\nAverage Item Price was: $7.46\n\n\n\n\nStep 17. How many different items are sold?\n\nchipo.item_name.nunique()\n\n50",
    "crumbs": [
      "Exercises",
      "Lecture",
      "lab1-1 chipotle"
    ]
  },
  {
    "objectID": "labs/lab1-3 World Food Facts .html",
    "href": "labs/lab1-3 World Food Facts .html",
    "title": "Exercise 1",
    "section": "",
    "text": "Step 1. Go to https://www.kaggle.com/openfoodfacts/world-food-facts/data\n\n\nStep 2. Download the dataset to your computer and unzip it.\n\n\nStep 3. Use the tsv file and assign it to a dataframe called food\n\nimport pandas as pd\nfile_path = 'data/en.openfoodfacts.org.products.tsv'\nfood = pd.read_csv(file_path, sep='\\t')\npd.set_option('display.width', 600) \n# 打印表头（列名）\nprint(food.columns.tolist())\n\nC:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8028\\392779500.py:3: DtypeWarning: Columns (0,3,5,19,20,24,25,26,27,28,36,37,38,39,48) have mixed types. Specify dtype option on import or set low_memory=False.\n  food = pd.read_csv(file_path, sep='\\t')\n\n\n['code', 'url', 'creator', 'created_t', 'created_datetime', 'last_modified_t', 'last_modified_datetime', 'product_name', 'generic_name', 'quantity', 'packaging', 'packaging_tags', 'brands', 'brands_tags', 'categories', 'categories_tags', 'categories_en', 'origins', 'origins_tags', 'manufacturing_places', 'manufacturing_places_tags', 'labels', 'labels_tags', 'labels_en', 'emb_codes', 'emb_codes_tags', 'first_packaging_code_geo', 'cities', 'cities_tags', 'purchase_places', 'stores', 'countries', 'countries_tags', 'countries_en', 'ingredients_text', 'allergens', 'allergens_en', 'traces', 'traces_tags', 'traces_en', 'serving_size', 'no_nutriments', 'additives_n', 'additives', 'additives_tags', 'additives_en', 'ingredients_from_palm_oil_n', 'ingredients_from_palm_oil', 'ingredients_from_palm_oil_tags', 'ingredients_that_may_be_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil', 'ingredients_that_may_be_from_palm_oil_tags', 'nutrition_grade_uk', 'nutrition_grade_fr', 'pnns_groups_1', 'pnns_groups_2', 'states', 'states_tags', 'states_en', 'main_category', 'main_category_en', 'image_url', 'image_small_url', 'energy_100g', 'energy-from-fat_100g', 'fat_100g', 'saturated-fat_100g', '-butyric-acid_100g', '-caproic-acid_100g', '-caprylic-acid_100g', '-capric-acid_100g', '-lauric-acid_100g', '-myristic-acid_100g', '-palmitic-acid_100g', '-stearic-acid_100g', '-arachidic-acid_100g', '-behenic-acid_100g', '-lignoceric-acid_100g', '-cerotic-acid_100g', '-montanic-acid_100g', '-melissic-acid_100g', 'monounsaturated-fat_100g', 'polyunsaturated-fat_100g', 'omega-3-fat_100g', '-alpha-linolenic-acid_100g', '-eicosapentaenoic-acid_100g', '-docosahexaenoic-acid_100g', 'omega-6-fat_100g', '-linoleic-acid_100g', '-arachidonic-acid_100g', '-gamma-linolenic-acid_100g', '-dihomo-gamma-linolenic-acid_100g', 'omega-9-fat_100g', '-oleic-acid_100g', '-elaidic-acid_100g', '-gondoic-acid_100g', '-mead-acid_100g', '-erucic-acid_100g', '-nervonic-acid_100g', 'trans-fat_100g', 'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g', '-sucrose_100g', '-glucose_100g', '-fructose_100g', '-lactose_100g', '-maltose_100g', '-maltodextrins_100g', 'starch_100g', 'polyols_100g', 'fiber_100g', 'proteins_100g', 'casein_100g', 'serum-proteins_100g', 'nucleotides_100g', 'salt_100g', 'sodium_100g', 'alcohol_100g', 'vitamin-a_100g', 'beta-carotene_100g', 'vitamin-d_100g', 'vitamin-e_100g', 'vitamin-k_100g', 'vitamin-c_100g', 'vitamin-b1_100g', 'vitamin-b2_100g', 'vitamin-pp_100g', 'vitamin-b6_100g', 'vitamin-b9_100g', 'folates_100g', 'vitamin-b12_100g', 'biotin_100g', 'pantothenic-acid_100g', 'silica_100g', 'bicarbonate_100g', 'potassium_100g', 'chloride_100g', 'calcium_100g', 'phosphorus_100g', 'iron_100g', 'magnesium_100g', 'zinc_100g', 'copper_100g', 'manganese_100g', 'fluoride_100g', 'selenium_100g', 'chromium_100g', 'molybdenum_100g', 'iodine_100g', 'caffeine_100g', 'taurine_100g', 'ph_100g', 'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-estimate_100g', 'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g', 'carbon-footprint_100g', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g', 'glycemic-index_100g', 'water-hardness_100g']\n\n\n### Step 4. See the first 5 entries\n\n\nfood.set_index('code', inplace=True)\n# 打印前5行数据\nprint(food.head(5))\n\n                                                     url                     creator   created_t      created_datetime last_modified_t last_modified_datetime                    product_name generic_name quantity packaging  ... fruits-vegetables-nuts_100g fruits-vegetables-nuts-estimate_100g collagen-meat-protein-ratio_100g cocoa_100g chlorophyl_100g carbon-footprint_100g nutrition-score-fr_100g nutrition-score-uk_100g glycemic-index_100g water-hardness_100g\ncode                                                                                                                                                                                                                           ...                                                                                                                                                                                                                                           \n3087   http://world-en.openfoodfacts.org/product/0000...  openfoodfacts-contributors  1474103866  2016-09-17T09:17:46Z      1474103893   2016-09-17T09:18:13Z              Farine de blé noir          NaN      1kg       NaN  ...                         NaN                                  NaN                              NaN        NaN             NaN                   NaN                     NaN                     NaN                 NaN                 NaN\n4530   http://world-en.openfoodfacts.org/product/0000...             usda-ndb-import  1489069957  2017-03-09T14:32:37Z      1489069957   2017-03-09T14:32:37Z  Banana Chips Sweetened (Whole)          NaN      NaN       NaN  ...                         NaN                                  NaN                              NaN        NaN             NaN                   NaN                    14.0                    14.0                 NaN                 NaN\n4559   http://world-en.openfoodfacts.org/product/0000...             usda-ndb-import  1489069957  2017-03-09T14:32:37Z      1489069957   2017-03-09T14:32:37Z                         Peanuts          NaN      NaN       NaN  ...                         NaN                                  NaN                              NaN        NaN             NaN                   NaN                     0.0                     0.0                 NaN                 NaN\n16087  http://world-en.openfoodfacts.org/product/0000...             usda-ndb-import  1489055731  2017-03-09T10:35:31Z      1489055731   2017-03-09T10:35:31Z          Organic Salted Nut Mix          NaN      NaN       NaN  ...                         NaN                                  NaN                              NaN        NaN             NaN                   NaN                    12.0                    12.0                 NaN                 NaN\n16094  http://world-en.openfoodfacts.org/product/0000...             usda-ndb-import  1489055653  2017-03-09T10:34:13Z      1489055653   2017-03-09T10:34:13Z                 Organic Polenta          NaN      NaN       NaN  ...                         NaN                                  NaN                              NaN        NaN             NaN                   NaN                     NaN                     NaN                 NaN                 NaN\n\n[5 rows x 162 columns]\n\n\n\n\nStep 5. What is the number of observations in the dataset?\n\nnum_rows = len(food)\nprint(f\"{num_rows}\")\nnum_columns = len(food.columns)\nprint(f\"{num_columns}\")\n\n356027\n162\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\nfood.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 356027 entries, 3087 to 999990026839\nColumns: 162 entries, url to water-hardness_100g\ndtypes: float64(107), object(55)\nmemory usage: 442.8+ MB\n\n\n\n\nStep 7. Print the name of all the columns.\n\nfood.columns\n\nIndex(['url', 'creator', 'created_t', 'created_datetime', 'last_modified_t', 'last_modified_datetime', 'product_name', 'generic_name', 'quantity', 'packaging',\n       ...\n       'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-estimate_100g', 'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g', 'carbon-footprint_100g', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g', 'glycemic-index_100g', 'water-hardness_100g'], dtype='object', length=162)\n\n\n\n\nStep 8. What is the name of 105th column?\n\ncolumn_name = food.columns[104]\nprint(f\"{column_name}\")\n\n-fructose_100g\n\n\n\n\nStep 9. What is the type of the observations of the 105th column?\n\ncolumn_data_type = food[column_name].dtype\nprint(column_data_type)\n\nfloat64\n\n\n\n\nStep 10. How is the dataset indexed?\n\nfood = pd.DataFrame({'data': range(356027)})  \n \nif isinstance(food.index, pd.RangeIndex):\n    start = food.index.start\n    stop = food.index.stop\n    step = food.index.step\n    \n    print(f\"RangeIndex(start={start}, stop={stop}, step={step})\")\n\nRangeIndex(start=0, stop=356027, step=1)\n\n\n\n\nStep 11. What is the product name of the 19th observation?\n\nproduct_name_19th_row = food.iloc[18]['product_name']\n \n# 打印结果\nprint(f\"{product_name_19th_row}\")\n\nLotus Organic Brown Jasmine Rice",
    "crumbs": [
      "Exercises",
      "Lecture",
      "lab1-3 World Food Facts"
    ]
  },
  {
    "objectID": "labs/lab1-5 Euro12.html",
    "href": "labs/lab1-5 Euro12.html",
    "title": "Ex2 - Filtering and Sorting Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called euro12.\n\neuro12 = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv')\npd.set_option('display.width', 400)  \nprint(euro12)\n\n                   Team  Goals  Shots on target  Shots off target Shooting Accuracy % Goals-to-shots  Total shots (inc. Blocked)  Hit Woodwork  Penalty goals  Penalties not scored  ...  Saves made  Saves-to-shots ratio  Fouls Won Fouls Conceded  Offsides  Yellow Cards  Red Cards  Subs on  Subs off  Players Used\n0               Croatia      4               13                12             51.9%            16.0%                          32             0              0                     0  ...          13                 81.3%         41             62         2             9          0        9         9            16\n1        Czech Republic      4               13                18             41.9%            12.9%                          39             0              0                     0  ...           9                 60.1%         53             73         8             7          0       11        11            19\n2               Denmark      4               10                10             50.0%            20.0%                          27             1              0                     0  ...          10                 66.7%         25             38         8             4          0        7         7            15\n3               England      5               11                18             50.0%            17.2%                          40             0              0                     0  ...          22                 88.1%         43             45         6             5          0       11        11            16\n4                France      3               22                24             37.9%             6.5%                          65             1              0                     0  ...           6                 54.6%         36             51         5             6          0       11        11            19\n5               Germany     10               32                32             47.8%            15.6%                          80             2              1                     0  ...          10                 62.6%         63             49        12             4          0       15        15            17\n6                Greece      5                8                18             30.7%            19.2%                          32             1              1                     1  ...          13                 65.1%         67             48        12             9          1       12        12            20\n7                 Italy      6               34                45             43.0%             7.5%                         110             2              0                     0  ...          20                 74.1%        101             89        16            16          0       18        18            19\n8           Netherlands      2               12                36             25.0%             4.1%                          60             2              0                     0  ...          12                 70.6%         35             30         3             5          0        7         7            15\n9                Poland      2               15                23             39.4%             5.2%                          48             0              0                     0  ...           6                 66.7%         48             56         3             7          1        7         7            17\n10             Portugal      6               22                42             34.3%             9.3%                          82             6              0                     0  ...          10                 71.5%         73             90        10            12          0       14        14            16\n11  Republic of Ireland      1                7                12             36.8%             5.2%                          28             0              0                     0  ...          17                 65.4%         43             51        11             6          1       10        10            17\n12               Russia      5                9                31             22.5%            12.5%                          59             2              0                     0  ...          10                 77.0%         34             43         4             6          0        7         7            16\n13                Spain     12               42                33             55.9%            16.0%                         100             0              1                     0  ...          15                 93.8%        102             83        19            11          0       17        17            18\n14               Sweden      5               17                19             47.2%            13.8%                          39             3              0                     0  ...           8                 61.6%         35             51         7             7          0        9         9            18\n15              Ukraine      2                7                26             21.2%             6.0%                          38             0              0                     0  ...          13                 76.5%         48             31         4             5          0        9         9            18\n\n[16 rows x 35 columns]\n\n\n\n\nStep 4. Select only the Goal column.\n\ngoals = euro12['Goals']\nprint(goals)\n\n0      4\n1      4\n2      4\n3      5\n4      3\n5     10\n6      5\n7      6\n8      2\n9      2\n10     6\n11     1\n12     5\n13    12\n14     5\n15     2\nName: Goals, dtype: int64\n\n\n\n\nStep 5. How many team participated in the Euro2012?\n\nnum_teams = euro12.shape[0]\nprint(f'{num_teams}')\n\n16\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\neuro12.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16 entries, 0 to 15\nData columns (total 35 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Team                        16 non-null     object \n 1   Goals                       16 non-null     int64  \n 2   Shots on target             16 non-null     int64  \n 3   Shots off target            16 non-null     int64  \n 4   Shooting Accuracy           16 non-null     object \n 5   % Goals-to-shots            16 non-null     object \n 6   Total shots (inc. Blocked)  16 non-null     int64  \n 7   Hit Woodwork                16 non-null     int64  \n 8   Penalty goals               16 non-null     int64  \n 9   Penalties not scored        16 non-null     int64  \n 10  Headed goals                16 non-null     int64  \n 11  Passes                      16 non-null     int64  \n 12  Passes completed            16 non-null     int64  \n 13  Passing Accuracy            16 non-null     object \n 14  Touches                     16 non-null     int64  \n 15  Crosses                     16 non-null     int64  \n 16  Dribbles                    16 non-null     int64  \n 17  Corners Taken               16 non-null     int64  \n 18  Tackles                     16 non-null     int64  \n 19  Clearances                  16 non-null     int64  \n 20  Interceptions               16 non-null     int64  \n 21  Clearances off line         15 non-null     float64\n 22  Clean Sheets                16 non-null     int64  \n 23  Blocks                      16 non-null     int64  \n 24  Goals conceded              16 non-null     int64  \n 25  Saves made                  16 non-null     int64  \n 26  Saves-to-shots ratio        16 non-null     object \n 27  Fouls Won                   16 non-null     int64  \n 28  Fouls Conceded              16 non-null     int64  \n 29  Offsides                    16 non-null     int64  \n 30  Yellow Cards                16 non-null     int64  \n 31  Red Cards                   16 non-null     int64  \n 32  Subs on                     16 non-null     int64  \n 33  Subs off                    16 non-null     int64  \n 34  Players Used                16 non-null     int64  \ndtypes: float64(1), int64(29), object(5)\nmemory usage: 4.5+ KB\n\n\n\n\nStep 7. View only the columns Team, Yellow Cards and Red Cards and assign them to a dataframe called discipline\n\ndiscipline = euro12[['Team', 'Yellow Cards', 'Red Cards']]\nprint(discipline)\n\n                   Team  Yellow Cards  Red Cards\n0               Croatia             9          0\n1        Czech Republic             7          0\n2               Denmark             4          0\n3               England             5          0\n4                France             6          0\n5               Germany             4          0\n6                Greece             9          1\n7                 Italy            16          0\n8           Netherlands             5          0\n9                Poland             7          1\n10             Portugal            12          0\n11  Republic of Ireland             6          1\n12               Russia             6          0\n13                Spain            11          0\n14               Sweden             7          0\n15              Ukraine             5          0\n\n\n\n\nStep 8. Sort the teams by Red Cards, then to Yellow Cards\n\nsorted_discipline = discipline.sort_values(by=['Red Cards', 'Yellow Cards'], ascending=[False, False])\nprint(sorted_discipline)\n\n                   Team  Yellow Cards  Red Cards\n6                Greece             9          1\n9                Poland             7          1\n11  Republic of Ireland             6          1\n7                 Italy            16          0\n10             Portugal            12          0\n13                Spain            11          0\n0               Croatia             9          0\n1        Czech Republic             7          0\n14               Sweden             7          0\n4                France             6          0\n12               Russia             6          0\n3               England             5          0\n8           Netherlands             5          0\n15              Ukraine             5          0\n2               Denmark             4          0\n5               Germany             4          0\n\n\n\n\nStep 9. Calculate the mean Yellow Cards given per Team\n\nmean_yellow_cards = round(euro12['Yellow Cards'].mean(), 1)\nprint(mean_yellow_cards)\n\n7.4\n\n\n\n\nStep 10. Filter teams that scored more than 6 goals\n\nteams_more_than_6_goals = euro12[euro12['Goals'] &gt; 6]\ngermany_spain = teams_more_than_6_goals[teams_more_than_6_goals['Team'].isin(['Germany', 'Spain'])]\nprint(germany_spain)\n\n       Team  Goals  Shots on target  Shots off target Shooting Accuracy % Goals-to-shots  Total shots (inc. Blocked)  Hit Woodwork  Penalty goals  Penalties not scored  ...  Saves made  Saves-to-shots ratio  Fouls Won Fouls Conceded  Offsides  Yellow Cards  Red Cards  Subs on  Subs off  Players Used\n5   Germany     10               32                32             47.8%            15.6%                          80             2              1                     0  ...          10                 62.6%         63             49        12             4          0       15        15            17\n13    Spain     12               42                33             55.9%            16.0%                         100             0              1                     0  ...          15                 93.8%        102             83        19            11          0       17        17            18\n\n[2 rows x 35 columns]\n\n\n\n\nStep 11. Select the teams that start with G\n\nteams_start_with_G = euro12[euro12['Team'].str.startswith('G')]\nprint(teams_start_with_G)\n\n      Team  Goals  Shots on target  Shots off target Shooting Accuracy % Goals-to-shots  Total shots (inc. Blocked)  Hit Woodwork  Penalty goals  Penalties not scored  ...  Saves made  Saves-to-shots ratio  Fouls Won Fouls Conceded  Offsides  Yellow Cards  Red Cards  Subs on  Subs off  Players Used\n5  Germany     10               32                32             47.8%            15.6%                          80             2              1                     0  ...          10                 62.6%         63             49        12             4          0       15        15            17\n6   Greece      5                8                18             30.7%            19.2%                          32             1              1                     1  ...          13                 65.1%         67             48        12             9          1       12        12            20\n\n[2 rows x 35 columns]\n\n\n\n\nStep 12. Select the first 7 columns\n\nfirst_7_columns = euro12.iloc[:, :7]\nprint(first_7_columns)\n\n                   Team  Goals  Shots on target  Shots off target Shooting Accuracy % Goals-to-shots  Total shots (inc. Blocked)\n0               Croatia      4               13                12             51.9%            16.0%                          32\n1        Czech Republic      4               13                18             41.9%            12.9%                          39\n2               Denmark      4               10                10             50.0%            20.0%                          27\n3               England      5               11                18             50.0%            17.2%                          40\n4                France      3               22                24             37.9%             6.5%                          65\n5               Germany     10               32                32             47.8%            15.6%                          80\n6                Greece      5                8                18             30.7%            19.2%                          32\n7                 Italy      6               34                45             43.0%             7.5%                         110\n8           Netherlands      2               12                36             25.0%             4.1%                          60\n9                Poland      2               15                23             39.4%             5.2%                          48\n10             Portugal      6               22                42             34.3%             9.3%                          82\n11  Republic of Ireland      1                7                12             36.8%             5.2%                          28\n12               Russia      5                9                31             22.5%            12.5%                          59\n13                Spain     12               42                33             55.9%            16.0%                         100\n14               Sweden      5               17                19             47.2%            13.8%                          39\n15              Ukraine      2                7                26             21.2%             6.0%                          38\n\n\n\n\nStep 13. Select all columns except the last 3.\n\ncolumns_excluding_last_3 = euro12.iloc[:, :-3]\nprint(columns_excluding_last_3)\n\n                   Team  Goals  Shots on target  Shots off target Shooting Accuracy % Goals-to-shots  Total shots (inc. Blocked)  Hit Woodwork  Penalty goals  Penalties not scored  ...  Clean Sheets  Blocks  Goals conceded Saves made  Saves-to-shots ratio  Fouls Won  Fouls Conceded  Offsides  Yellow Cards  Red Cards\n0               Croatia      4               13                12             51.9%            16.0%                          32             0              0                     0  ...             0      10               3         13                 81.3%         41              62         2             9          0\n1        Czech Republic      4               13                18             41.9%            12.9%                          39             0              0                     0  ...             1      10               6          9                 60.1%         53              73         8             7          0\n2               Denmark      4               10                10             50.0%            20.0%                          27             1              0                     0  ...             1      10               5         10                 66.7%         25              38         8             4          0\n3               England      5               11                18             50.0%            17.2%                          40             0              0                     0  ...             2      29               3         22                 88.1%         43              45         6             5          0\n4                France      3               22                24             37.9%             6.5%                          65             1              0                     0  ...             1       7               5          6                 54.6%         36              51         5             6          0\n5               Germany     10               32                32             47.8%            15.6%                          80             2              1                     0  ...             1      11               6         10                 62.6%         63              49        12             4          0\n6                Greece      5                8                18             30.7%            19.2%                          32             1              1                     1  ...             1      23               7         13                 65.1%         67              48        12             9          1\n7                 Italy      6               34                45             43.0%             7.5%                         110             2              0                     0  ...             2      18               7         20                 74.1%        101              89        16            16          0\n8           Netherlands      2               12                36             25.0%             4.1%                          60             2              0                     0  ...             0       9               5         12                 70.6%         35              30         3             5          0\n9                Poland      2               15                23             39.4%             5.2%                          48             0              0                     0  ...             0       8               3          6                 66.7%         48              56         3             7          1\n10             Portugal      6               22                42             34.3%             9.3%                          82             6              0                     0  ...             2      11               4         10                 71.5%         73              90        10            12          0\n11  Republic of Ireland      1                7                12             36.8%             5.2%                          28             0              0                     0  ...             0      23               9         17                 65.4%         43              51        11             6          1\n12               Russia      5                9                31             22.5%            12.5%                          59             2              0                     0  ...             0       8               3         10                 77.0%         34              43         4             6          0\n13                Spain     12               42                33             55.9%            16.0%                         100             0              1                     0  ...             5       8               1         15                 93.8%        102              83        19            11          0\n14               Sweden      5               17                19             47.2%            13.8%                          39             3              0                     0  ...             1      12               5          8                 61.6%         35              51         7             7          0\n15              Ukraine      2                7                26             21.2%             6.0%                          38             0              0                     0  ...             0       4               4         13                 76.5%         48              31         4             5          0\n\n[16 rows x 32 columns]\n\n\n\n\nStep 14. Present only the Shooting Accuracy from England, Italy and Russia\n\nshooting_accuracy = euro12[euro12['Team'].isin(['England', 'Italy', 'Russia'])][['Team', 'Shooting Accuracy']]\nprint(shooting_accuracy)\n\n       Team Shooting Accuracy\n3   England             50.0%\n7     Italy             43.0%\n12   Russia             22.5%",
    "crumbs": [
      "Exercises",
      "Lecture",
      "lab1-5 Euro12"
    ]
  },
  {
    "objectID": "labs/lab1-7 Scores.html",
    "href": "labs/lab1-7 Scores.html",
    "title": "Scores",
    "section": "",
    "text": "Introduction:\nThis time you will create the data.\nExercise based on Chris Albon work, the credits belong to him.\n\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\n\n\nStep 2. Create the DataFrame that should look like the one below.\n\ndata = {\n    'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],\n    'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'],\n    'age': [42, 52, 36, 24, 73],\n    'female': [0, 1, 1, 0, 1],\n    'preTestScore': [4, 24, 31, 2, 3],\n    'postTestScore': [25, 94, 57, 62, 70]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n  first_name last_name  age  female  preTestScore  postTestScore\n0      Jason    Miller   42       0             4             25\n1      Molly  Jacobson   52       1            24             94\n2       Tina       Ali   36       1            31             57\n3       Jake    Milner   24       0             2             62\n4        Amy     Cooze   73       1             3             70\n\n\n\n\nStep 3. Create a Scatterplot of preTestScore and postTestScore, with the size of each point determined by age\n\nHint: Don’t forget to place the labels\n\nplt.scatter(df['preTestScore'], df['postTestScore'], s=df['age'], alpha=0.5)\n\n# Adding labels\nplt.xlabel('Pre Test Score')\nplt.ylabel('Post Test Score')\nplt.title('Pre Test Score vs Post Test Score')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStep 4. Create a Scatterplot of preTestScore and postTestScore.\n\n\nThis time the size should be 4.5 times the postTestScore and the color determined by sex\n\nplt.scatter(df['preTestScore'], df['postTestScore'], \n            s=4.5 * df['postTestScore'], c=df['female'], cmap='coolwarm', alpha=0.5)\n\n# Adding labels\nplt.xlabel('Pre Test Score')\nplt.ylabel('Post Test Score')\nplt.title('Pre Test Score vs Post Test Score with Size and Color')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBONUS: Create your own question and answer it.\n\ncorrelation = df['preTestScore'].corr(df['postTestScore'])\nprint(f\"Correlation between preTestScore and postTestScore: {correlation}\")\n\nCorrelation between preTestScore and postTestScore: 0.37803911777651406",
    "crumbs": [
      "Exercises",
      "Lecture",
      "lab1-7 Scores"
    ]
  },
  {
    "objectID": "Lecture/homework 1 for lecture 3.html",
    "href": "Lecture/homework 1 for lecture 3.html",
    "title": "Tan Yan website",
    "section": "",
    "text": "import pandas as pd\nurl ='https://raw.githubusercontent.com/tidyverse/datascience-box/refs/heads/main/course-materials/lab-instructions/lab-03/data/nobel.csv'\ndf = pd.read_csv(url)\nprint(df.head())\n\n   id       firstname    surname  year category  \\\n0   1  Wilhelm Conrad    Röntgen  1901  Physics   \n1   2      Hendrik A.    Lorentz  1902  Physics   \n2   3          Pieter     Zeeman  1902  Physics   \n3   4           Henri  Becquerel  1903  Physics   \n4   5          Pierre      Curie  1903  Physics   \n\n                                         affiliation       city      country  \\\n0                                  Munich University     Munich      Germany   \n1                                  Leiden University     Leiden  Netherlands   \n2                               Amsterdam University  Amsterdam  Netherlands   \n3                                École Polytechnique      Paris       France   \n4  École municipale de physique et de chimie indu...      Paris       France   \n\n    born_date   died_date  ... died_country_code overall_motivation share  \\\n0  1845-03-27  1923-02-10  ...                DE                NaN     1   \n1  1853-07-18  1928-02-04  ...                NL                NaN     2   \n2  1865-05-25  1943-10-09  ...                NL                NaN     2   \n3  1852-12-15  1908-08-25  ...                FR                NaN     2   \n4  1859-05-15  1906-04-19  ...                FR                NaN     4   \n\n                                          motivation  born_country_original  \\\n0  \"in recognition of the extraordinary services ...  Prussia (now Germany)   \n1  \"in recognition of the extraordinary service t...        the Netherlands   \n2  \"in recognition of the extraordinary service t...        the Netherlands   \n3  \"in recognition of the extraordinary services ...                 France   \n4  \"in recognition of the extraordinary services ...                 France   \n\n       born_city_original died_country_original died_city_original  \\\n0  Lennep (now Remscheid)               Germany             Munich   \n1                  Arnhem       the Netherlands                NaN   \n2              Zonnemaire       the Netherlands          Amsterdam   \n3                   Paris                France                NaN   \n4                   Paris                France              Paris   \n\n   city_original country_original  \n0         Munich          Germany  \n1         Leiden  the Netherlands  \n2      Amsterdam  the Netherlands  \n3          Paris           France  \n4          Paris           France  \n\n[5 rows x 26 columns]\n\n\n\ndf\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n0\n1\nWilhelm Conrad\nRöntgen\n1901\nPhysics\nMunich University\nMunich\nGermany\n1845-03-27\n1923-02-10\n...\nDE\nNaN\n1\n\"in recognition of the extraordinary services ...\nPrussia (now Germany)\nLennep (now Remscheid)\nGermany\nMunich\nMunich\nGermany\n\n\n1\n2\nHendrik A.\nLorentz\n1902\nPhysics\nLeiden University\nLeiden\nNetherlands\n1853-07-18\n1928-02-04\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nArnhem\nthe Netherlands\nNaN\nLeiden\nthe Netherlands\n\n\n2\n3\nPieter\nZeeman\n1902\nPhysics\nAmsterdam University\nAmsterdam\nNetherlands\n1865-05-25\n1943-10-09\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nZonnemaire\nthe Netherlands\nAmsterdam\nAmsterdam\nthe Netherlands\n\n\n3\n4\nHenri\nBecquerel\n1903\nPhysics\nÉcole Polytechnique\nParis\nFrance\n1852-12-15\n1908-08-25\n...\nFR\nNaN\n2\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nNaN\nParis\nFrance\n\n\n4\n5\nPierre\nCurie\n1903\nPhysics\nÉcole municipale de physique et de chimie indu...\nParis\nFrance\n1859-05-15\n1906-04-19\n...\nFR\nNaN\n4\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nParis\nParis\nFrance\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n931\n966\nDenis\nMukwege\n2018\nPeace\nNaN\nNaN\nNaN\n1955-03-01\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nBelgian Congo (now Democratic Republic of the ...\nBukavu\nNaN\nNaN\nNaN\nNaN\n\n\n932\n967\nNadia\nMurad\n2018\nPeace\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nIraq\nKojo\nNaN\nNaN\nNaN\nNaN\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n935 rows × 26 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 935 entries, 0 to 934\nData columns (total 26 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   id                     935 non-null    int64 \n 1   firstname              935 non-null    object\n 2   surname                906 non-null    object\n 3   year                   935 non-null    int64 \n 4   category               935 non-null    object\n 5   affiliation            685 non-null    object\n 6   city                   680 non-null    object\n 7   country                681 non-null    object\n 8   born_date              902 non-null    object\n 9   died_date              627 non-null    object\n 10  gender                 935 non-null    object\n 11  born_city              907 non-null    object\n 12  born_country           907 non-null    object\n 13  born_country_code      907 non-null    object\n 14  died_city              608 non-null    object\n 15  died_country           614 non-null    object\n 16  died_country_code      614 non-null    object\n 17  overall_motivation     17 non-null     object\n 18  share                  935 non-null    int64 \n 19  motivation             935 non-null    object\n 20  born_country_original  907 non-null    object\n 21  born_city_original     907 non-null    object\n 22  died_country_original  614 non-null    object\n 23  died_city_original     608 non-null    object\n 24  city_original          680 non-null    object\n 25  country_original       681 non-null    object\ndtypes: int64(3), object(23)\nmemory usage: 190.1+ KB\n\n\n\ndf[(df['country'].notna())]\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n0\n1\nWilhelm Conrad\nRöntgen\n1901\nPhysics\nMunich University\nMunich\nGermany\n1845-03-27\n1923-02-10\n...\nDE\nNaN\n1\n\"in recognition of the extraordinary services ...\nPrussia (now Germany)\nLennep (now Remscheid)\nGermany\nMunich\nMunich\nGermany\n\n\n1\n2\nHendrik A.\nLorentz\n1902\nPhysics\nLeiden University\nLeiden\nNetherlands\n1853-07-18\n1928-02-04\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nArnhem\nthe Netherlands\nNaN\nLeiden\nthe Netherlands\n\n\n2\n3\nPieter\nZeeman\n1902\nPhysics\nAmsterdam University\nAmsterdam\nNetherlands\n1865-05-25\n1943-10-09\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nZonnemaire\nthe Netherlands\nAmsterdam\nAmsterdam\nthe Netherlands\n\n\n3\n4\nHenri\nBecquerel\n1903\nPhysics\nÉcole Polytechnique\nParis\nFrance\n1852-12-15\n1908-08-25\n...\nFR\nNaN\n2\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nNaN\nParis\nFrance\n\n\n4\n5\nPierre\nCurie\n1903\nPhysics\nÉcole municipale de physique et de chimie indu...\nParis\nFrance\n1859-05-15\n1906-04-19\n...\nFR\nNaN\n4\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nParis\nParis\nFrance\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n928\n963\nFrances H.\nArnold\n2018\nChemistry\nCalifornia Institute of Technology (Caltech)\nPasadena CA\nUSA\n1956-07-25\nNaN\n...\nNaN\nNaN\n2\n\"for the directed evolution of enzymes\"\nUSA\nPittsburgh PA\nNaN\nNaN\nPasadena CA\nUSA\n\n\n929\n964\nGeorge P.\nSmith\n2018\nChemistry\nUniversity of Missouri\nColumbia\nUSA\n1941-03-10\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUSA\nNorwalk CT\nNaN\nNaN\nColumbia\nUSA\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n681 rows × 26 columns\n\n\n\n\ndf[(df['died_date'].isna())]\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n68\n68\nChen Ning\nYang\n1957\nPhysics\nInstitute for Advanced Study\nPrinceton NJ\nUSA\n1922-09-22\nNaN\n...\nNaN\nNaN\n2\n\"for their penetrating investigation of the so...\nChina\nHofei Anhwei\nNaN\nNaN\nPrinceton NJ\nUSA\n\n\n69\n69\nTsung-Dao\nLee\n1957\nPhysics\nColumbia University\nNew York NY\nUSA\n1926-11-24\nNaN\n...\nNaN\nNaN\n2\n\"for their penetrating investigation of the so...\nChina\nShanghai\nNaN\nNaN\nNew York NY\nUSA\n\n\n94\n95\nLeon N.\nCooper\n1972\nPhysics\nBrown University\nProvidence RI\nUSA\n1930-02-28\nNaN\n...\nNaN\nNaN\n3\n\"for their jointly developed theory of superco...\nUSA\nNew York NY\nNaN\nNaN\nProvidence RI\nUSA\n\n\n96\n97\nLeo\nEsaki\n1973\nPhysics\nIBM Thomas J. Watson Research Center\nYorktown Heights NY\nUSA\n1925-03-12\nNaN\n...\nNaN\nNaN\n4\n\"for their experimental discoveries regarding ...\nJapan\nOsaka\nNaN\nNaN\nYorktown Heights NY\nUSA\n\n\n97\n98\nIvar\nGiaever\n1973\nPhysics\nGeneral Electric Company\nSchenectady NY\nUSA\n1929-04-05\nNaN\n...\nNaN\nNaN\n4\n\"for their experimental discoveries regarding ...\nNorway\nBergen\nNaN\nNaN\nSchenectady NY\nUSA\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n931\n966\nDenis\nMukwege\n2018\nPeace\nNaN\nNaN\nNaN\n1955-03-01\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nBelgian Congo (now Democratic Republic of the ...\nBukavu\nNaN\nNaN\nNaN\nNaN\n\n\n932\n967\nNadia\nMurad\n2018\nPeace\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nIraq\nKojo\nNaN\nNaN\nNaN\nNaN\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n308 rows × 26 columns",
    "crumbs": [
      "Exercises",
      "Labs",
      "homework 1 for lecture 3"
    ]
  },
  {
    "objectID": "Lecture/homework for lecture 2-pet names.html",
    "href": "Lecture/homework for lecture 2-pet names.html",
    "title": "Tan Yan website",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv('data/seattle_pet_licenses.csv')\n\n\ndf\n\n\n\n\n\n\n\n\nanimal_s_name\nlicense_issue_date\nlicense_number\nprimary_breed\nsecondary_breed\nspecies\nzip_code\n\n\n\n\n0\nOzzy\n2005-03-29T00:00:00.000\n130651.0\nDachshund, Standard Smooth Haired\nNaN\nDog\n98104\n\n\n1\nJack\n2009-12-23T00:00:00.000\n898148.0\nSchnauzer, Miniature\nTerrier, Rat\nDog\n98107\n\n\n2\nGinger\n2006-01-20T00:00:00.000\n29654.0\nRetriever, Golden\nRetriever, Labrador\nDog\n98117\n\n\n3\nPepper\n2006-02-07T00:00:00.000\n75432.0\nManx\nMix\nCat\n98103\n\n\n4\nAddy\n2006-08-04T00:00:00.000\n729899.0\nRetriever, Golden\nNaN\nDog\n98105\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n66037\nLily\n2016-12-27T00:00:00.000\nNaN\nDomestic Shorthair\nMix\nCat\n98117\n\n\n66038\nEllie\n2016-11-29T00:00:00.000\nNaN\nGerman Shepherd\nMix\nDog\n98105\n\n\n66039\nSammy\n2016-12-05T00:00:00.000\nNaN\nTerrier\nMaltese\nDog\n98105\n\n\n66040\nBuddy\n2016-12-06T00:00:00.000\nNaN\nBullmastiff\nMix\nDog\n98105\n\n\n66041\nAku\n2016-12-07T00:00:00.000\nNaN\nChihuahua, Short Coat\nTerrier\nDog\n98106\n\n\n\n\n66042 rows × 7 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 66042 entries, 0 to 66041\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   animal_s_name       64685 non-null  object \n 1   license_issue_date  66042 non-null  object \n 2   license_number      43885 non-null  float64\n 3   primary_breed       66042 non-null  object \n 4   secondary_breed     22538 non-null  object \n 5   species             66042 non-null  object \n 6   zip_code            65884 non-null  object \ndtypes: float64(1), object(6)\nmemory usage: 3.5+ MB\n\n\n\ndf['animal_s_name'].value_counts()\n\nanimal_s_name\nLucy          566\nBella         451\nCharlie       447\nMax           374\nLuna          361\n             ... \nManasseh        1\nTaba            1\nMiriam          1\nNumber Six      1\nRollins         1\nName: count, Length: 15795, dtype: int64",
    "crumbs": [
      "Exercises",
      "Labs",
      "homework for lecture 2-pet names"
    ]
  },
  {
    "objectID": "Lecture/homework for lecture 5.html",
    "href": "Lecture/homework for lecture 5.html",
    "title": "Tan Yan website",
    "section": "",
    "text": "import pandas as pd\nurl ='https://raw.githubusercontent.com/tidyverse/datascience-box/refs/heads/main/course-materials/lab-instructions/lab-02/data/plastic-waste.csv'\ndf = pd.read_csv(url)\ndf\n\n\n\n\n\n\n\n\ncode\nentity\ncontinent\nyear\ngdp_per_cap\nplastic_waste_per_cap\nmismanaged_plastic_waste_per_cap\nmismanaged_plastic_waste\ncoastal_pop\ntotal_pop\n\n\n\n\n0\nAFG\nAfghanistan\nAsia\n2010\n1614.255001\nNaN\nNaN\nNaN\nNaN\n31411743.0\n\n\n1\nALB\nAlbania\nEurope\n2010\n9927.181841\n0.069\n0.032\n29705.0\n2530533.0\n3204284.0\n\n\n2\nDZA\nAlgeria\nAfrica\n2010\n12870.602699\n0.144\n0.086\n520555.0\n16556580.0\n35468208.0\n\n\n3\nASM\nAmerican Samoa\nOceania\n2010\nNaN\nNaN\nNaN\nNaN\nNaN\n68420.0\n\n\n4\nAND\nAndorra\nEurope\n2010\nNaN\nNaN\nNaN\nNaN\nNaN\n84864.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n235\nVNM\nVietnam\nAsia\n2010\n4408.168612\n0.103\n0.090\n1833819.0\n55858245.0\n87848445.0\n\n\n236\nESH\nWestern Sahara\nAfrica\n2010\nNaN\nNaN\nNaN\nNaN\nNaN\n530500.0\n\n\n237\nYEM\nYemen\nAsia\n2010\n4478.743599\n0.103\n0.077\n169181.0\n6048920.0\nNaN\n\n\n238\nZMB\nZambia\nAfrica\n2010\n3279.277161\nNaN\nNaN\nNaN\nNaN\n13088570.0\n\n\n239\nZWE\nZimbabwe\nAfrica\n2010\n1474.877128\nNaN\nNaN\nNaN\nNaN\n12571454.0\n\n\n\n\n240 rows × 10 columns\n\n\n\n\nimport pandas as pd\nfrom lets_plot import *\n\nLetsPlot.setup_html()\n\n\n            \n            \n            \n\n\n\n(\n    ggplot(df, aes(\"continent\", \"plastic_waste_per_cap\")) +\n    geom_histogram()\n)\n\n   \n   \n\n\n\n(\n    ggplot(df, aes(\"continent\", \"plastic_waste_per_cap\"))\n    + geom_boxplot()\n)\n\n   \n   \n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 利用violinplot函数绘制小提琴图\nsns.violinplot(x=df[\"continent\"], y=df[\"plastic_waste_per_cap\"])\n\nplt.show()\n\n\n\n\n\n\n\n\n\n(\n    ggplot(df, aes(\"plastic_waste_per_cap\", \"mismanaged_plastic_waste_per_cap\")) +\n  geom_point()\n)\n\n   \n   \n\n\n\n(\n    ggplot(df, aes(\"plastic_waste_per_cap\", \"mismanaged_plastic_waste_per_cap\", colour=\"continent\")) +\n  geom_point()\n)\n\n   \n   \n\n\n\n(\nggplot(df, aes(x=\"total_pop\", y=\"plastic_waste_per_cap\", colour=\"continent\")) +\n  geom_point() +\n  xlim(0, 150000000) +\n  ylim(0, 0.8)\n)\n\n   \n   \n\n\n\n(\nggplot(df, aes(x=\"coastal_pop\", y=\"plastic_waste_per_cap\", colour=\"continent\")) +\n  geom_point() +\n  xlim(0, 50000000) +\n  ylim(0, 0.8)\n)",
    "crumbs": [
      "Exercises",
      "Labs",
      "homework for lecture 5"
    ]
  }
]